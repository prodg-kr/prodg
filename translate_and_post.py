#!/usr/bin/env python3
"""
pronews.jp ìë™ ë²ˆì—­ ì‹œìŠ¤í…œ v7.2 (ë…¸ì´ì¦ˆ ì œê±° ë° HTML ìœ ì§€ ì™„ë²½ íŒ¨ì¹˜)
íŒŒì´í”„ë¼ì¸: ì¼ë³¸ì–´ ì›ë¬¸ â†’ Gemini 1íšŒ JSON í†µí•© ë²ˆì—­ â†’ WordPress Draft

v6 â†’ v7.2 ë³€ê²½ì‚¬í•­:
- fetch_full_content ë°˜í™˜ê°’ str(content_div) ë³€ê²½ (ë³¸ë¬¸ ì´ë¯¸ì§€ ìœ ì§€)
- ë²ˆì—­ í”„ë¡¬í”„íŠ¸ HTML íƒœê·¸ ìœ ì§€ ì§€ì‹œ ë° ê¸€ììˆ˜(15000) í•œë„ í™•ì¥
- ì‚¬ì´ë“œë°”, SNS ê³µìœ ë²„íŠ¼, ê´€ë ¨ê¸°ì‚¬ ë“± ë¶ˆí•„ìš”í•œ UI(Noise) ì™„ë²½ ì œê±°
- ëª¨ë¸: gemini-2.5-flash-lite (RPM 15, RPD 1,000)
- í˜¸ì¶œ êµ¬ì¡°: ê¸°ì‚¬ë‹¹ 1íšŒ JSON í†µí•© (TPM ì ˆê°, ì²˜ë¦¬ëŸ‰ ê·¹ëŒ€í™”)
- ì¬ë²ˆì—­: ì¼ë³¸ì–´ ì”ì¡´ ì‹œ ìµœëŒ€ 1íšŒ ì¶”ê°€ (ì´ 2íšŒ ìƒí•œ)
- Slug: ì •ê·œì‹ ëŒ€ì²´
- 429 ì²˜ë¦¬: ì§€ìˆ˜ ë°±ì˜¤í”„ í›„ ì¦‰ì‹œ ëŸ° ì¢…ë£Œ, ë¯¸ê¸°ë¡ â†’ ë‹¤ìŒ ëŸ° ìë™ ì´ì›”
- ì¼ë³¸ì–´ ì”ì¡´ ìŠ¤í‚µ ì œê±°: ê²½ê³  í›„ ë¬´ì¡°ê±´ ê²Œì‹œ
- ì‹¤í–‰ ëª¨ë“œ ë¶„ë¦¬: schedule(ìµœì‹  ìš°ì„ ), workflow_dispatch(ì•„ì¹´ì´ë¸Œ ìš°ì„ )
- ì•„ì¹´ì´ë¸Œ í¬ë¡¤ë§: /news/page/N/ í˜ì´ì§€ë„¤ì´ì…˜
- API í˜¸ì¶œ ê°„ê²©: 7ì´ˆ / ê¸°ì‚¬ ê°„ ëŒ€ê¸°: 10ì´ˆ
- POST_STATUS ê¸°ë³¸ê°’: draft
"""

import os
import sys
import requests
import feedparser
from datetime import datetime
from pathlib import Path
import json
import time
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
import hashlib
import re

# ==========================================
# ì„¤ì •
# ==========================================
WORDPRESS_URL          = "https://prodg.kr"
WORDPRESS_USER         = os.environ.get("WP_USER")
WORDPRESS_APP_PASSWORD = os.environ.get("WP_APP_PASSWORD")
GEMINI_API_KEY         = os.environ.get("GEMINI_API_KEY")
PRONEWS_RSS            = "https://jp.pronews.com/feed"
PRONEWS_ARCHIVE_BASE   = "https://jp.pronews.com/news/page"
POSTED_ARTICLES_FILE   = "posted_articles.json"
FORCE_UPDATE           = os.environ.get("FORCE_UPDATE", "false").lower() == "true"
DAILY_LIMIT            = 10
ARCHIVE_MAX_PAGES      = 20

# ì‹¤í–‰ ëª¨ë“œ ê°ì§€
GITHUB_EVENT_NAME = os.environ.get("GITHUB_EVENT_NAME", "workflow_dispatch")
IS_SCHEDULED      = GITHUB_EVENT_NAME == "schedule"

# ê²Œì‹œ ìƒíƒœ
POST_STATUS  = os.environ.get("POST_STATUS", "draft")
GEMINI_MODEL = os.environ.get("GEMINI_MODEL", "gemini-2.5-flash-lite")


# ==========================================
# Gemini í†µí•© ì—”ì§„
# ==========================================
class GeminiEngine:
    def __init__(self):
        self.api_key         = GEMINI_API_KEY
        if not self.api_key:
            print("âŒ GEMINI_API_KEY ë¯¸ì„¤ì •")
            sys.exit(1)
        self.last_call_time  = 0.0
        self.rate_limit_hit  = False

    def _call_api(self, prompt: str, max_tokens: int = 8192) -> str:
        if self.rate_limit_hit:
            return ""

        # í˜¸ì¶œ ê°„ê²© ë³´ì¥ (7ì´ˆ)
        elapsed = time.time() - self.last_call_time
        if elapsed < 7:
            time.sleep(7 - elapsed)
        self.last_call_time = time.time()

        url = (
            f"https://generativelanguage.googleapis.com/v1beta/models/"
            f"{GEMINI_MODEL}:generateContent?key={self.api_key}"
        )
        payload = {
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {
                "maxOutputTokens": max_tokens,
                "temperature": 0.4
            }
        }

        backoff = [15, 30, 60]
        for attempt in range(3):
            try:
                res = requests.post(url, json=payload, timeout=120)

                if res.status_code == 429:
                    wait = backoff[min(attempt, len(backoff) - 1)]
                    print(f"âš ï¸ 429 Rate Limit (ì‹œë„ {attempt+1}/3) â†’ {wait}ì´ˆ ëŒ€ê¸°...")
                    time.sleep(wait)
                    if attempt == 2:
                        print("âŒ 429 ë°˜ë³µ â†’ ëŸ° ì¢…ë£Œ (ë¯¸ê²Œì‹œ ê¸°ì‚¬ëŠ” ë‹¤ìŒ ëŸ° ìë™ ì´ì›”)")
                        self.rate_limit_hit = True
                        return ""
                    continue

                res.raise_for_status()
                candidates = res.json().get("candidates", [])
                if candidates:
                    parts = candidates[0]["content"]["parts"]
                    for part in parts:
                        if not part.get("thought", False) and "text" in part:
                            return part["text"].strip()
                    for part in reversed(parts):
                        if "text" in part:
                            return part["text"].strip()

                print(f"âš ï¸ Gemini ì‘ë‹µ ì—†ìŒ (ì‹œë„ {attempt+1}/3)")

            except Exception as e:
                print(f"âš ï¸ Gemini API ì˜¤ë¥˜ (ì‹œë„ {attempt+1}/3): {e}")
                if attempt < 2:
                    time.sleep(backoff[attempt])

        return ""

    def translate_article(self, title_ja: str, body_text: str) -> dict:
        prompt = f"""ë‹¹ì‹ ì€ ì˜ìƒ/ì¹´ë©”ë¼ ì „ë¬¸ ë¯¸ë””ì–´ì˜ í•œêµ­ì–´ ì—ë””í„°ì…ë‹ˆë‹¤.
ì•„ë˜ ì¼ë³¸ì–´ ê¸°ì‚¬(HTML)ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­Â·í¸ì§‘í•˜ì—¬ JSONìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.

=== ì¼ë³¸ì–´ ì›ë¬¸ ===
ì œëª©: {title_ja}

ë³¸ë¬¸:
{body_text[:15000]}

=== ë²ˆì—­ ê·œì¹™ ===
1. ì¼ë³¸ì–´(íˆë¼ê°€ë‚˜Â·ê°€íƒ€ì¹´ë‚˜Â·í•œì)ë¥¼ ì™„ì „íˆ í•œêµ­ì–´ë¡œ ë²ˆì—­
2. ë¬¸ì²´: ë°˜ë“œì‹œ '~í•©ë‹ˆë‹¤', '~í–ˆìŠµë‹ˆë‹¤', '~ì…ë‹ˆë‹¤' í•©ì‡¼ì²´ í†µì¼
3. ë¸Œëœë“œëª…Â·ëª¨ë¸ëª… ì›ë¬¸ ìœ ì§€: Sony, Canon, Nikon, DJI, Blackmagic, Sigma ë“±
4. í•´ìƒë„: 4K, 8K, Full HD / í”„ë ˆì„ë ˆì´íŠ¸: fps, 24p, 60p
5. â˜…ì¤‘ìš”â˜…: ë³¸ë¬¸ì— í¬í•¨ëœ <img>, <figure>, <iframe> ë“±ì˜ HTML ë¯¸ë””ì–´ íƒœê·¸ì™€ ì†ì„±(src, alt ë“±)ì€ ì ˆëŒ€ ì‚­ì œí•˜ê±°ë‚˜ ìˆ˜ì •í•˜ì§€ ë§ê³  ì œìë¦¬ì— ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”.
6. ê¸°ê³„ ë²ˆì—­ ëŠë‚Œ ì—†ì´ ì‚¬ëŒì´ ì“´ ë“¯ ìì—°ìŠ¤ëŸ½ê²Œ (Google SEOÂ·AdSense í’ˆì§ˆ ê¸°ì¤€)

=== ì¶œë ¥ JSON ê·œì¹™ ===
- title: SEO ìµœì í™” ì œëª© (ë¸Œëœë“œëª…Â·ëª¨ë¸ëª… í•„ìˆ˜ í¬í•¨, ìµœëŒ€ 50ì)
- content: ë²ˆì—­ ë³¸ë¬¸ (ì›ë³¸ HTML êµ¬ì¡° ë° ì´ë¯¸ì§€ íƒœê·¸ ì™„ë²½ ìœ ì§€)
- excerpt: êµ¬ê¸€ ìŠ¤ë‹ˆí«ìš© ìš”ì•½ (80~100ì, í•©ì‡¼ì²´)
- tldr: í•µì‹¬ ìš”ì•½ 3~4í•­ëª© (<ul><li> HTML, í•©ì‡¼ì²´)
- ë§ˆí¬ë‹¤ìš´ ë°±í‹± ì—†ì´ JSONë§Œ ì¶œë ¥

{{
  "title": "SEO ì œëª©",
  "content": "<p>ë³¸ë¬¸</p> <figure><img src='...'></figure>",
  "excerpt": "ìš”ì•½ë¬¸",
  "tldr": "<ul><li>ìš”ì•½1</li><li>ìš”ì•½2</li><li>ìš”ì•½3</li></ul>"
}}"""

        result = self._call_api(prompt, max_tokens=8192)
        if not result:
            return {}

        try:
            clean = re.sub(r'```(?:json)?', '', result).strip().rstrip('`').strip()
            match = re.search(r'(\{.*\})', clean, re.DOTALL)
            if match:
                return json.loads(match.group(1))
        except Exception as e:
            print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e} | ì›ë¬¸: {result[:200]}")

        return {}

    def retranslate_content(self, content_ko: str) -> str:
        prompt = f"""ì•„ë˜ í•œêµ­ì–´ ë³¸ë¬¸(HTML í¬í•¨)ì— ì¼ë³¸ì–´ê°€ ì„ì—¬ ìˆìŠµë‹ˆë‹¤.
ì¼ë³¸ì–´ ë¶€ë¶„ì„ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ í•©ì‡¼ì²´ë¡œ ë²ˆì—­í•˜ê³  ì „ì²´ ë³¸ë¬¸ì„ ë°˜í™˜í•˜ì„¸ìš”.
â˜…ì¤‘ìš”â˜… <img>, <figure> ë“± ëª¨ë“  HTML íƒœê·¸ì™€ ì†ì„±ì€ ì ˆëŒ€ ê±´ë“œë¦¬ì§€ ë§ê³  ê·¸ëŒ€ë¡œ ìœ ì§€í•  ê²ƒ. 
ë³¸ë¬¸ë§Œ ì¶œë ¥:

{content_ko[:15000]}"""
        result = self._call_api(prompt, max_tokens=8192)
        return result if result else content_ko

    def _has_japanese(self, text: str) -> bool:
        plain = BeautifulSoup(text, 'lxml').get_text()
        return len(re.findall(r'[\u3040-\u309f\u30a0-\u30ff]', plain)) > 5


# ==========================================
# ë©”ì¸ ë²ˆì—­ ì‹œìŠ¤í…œ
# ==========================================
class NewsTranslator:
    def __init__(self):
        self.gemini          = GeminiEngine()
        self.wordpress_api   = f"{WORDPRESS_URL}/wp-json/wp/v2"
        self.posted_articles = self.load_posted_articles()

    def load_posted_articles(self) -> list:
        if Path(POSTED_ARTICLES_FILE).exists():
            with open(POSTED_ARTICLES_FILE, 'r') as f:
                try:
                    return json.load(f)
                except:
                    return []
        return []

    def save_posted_articles(self):
        with open(POSTED_ARTICLES_FILE, 'w') as f:
            json.dump(self.posted_articles, f, indent=2)

    def fetch_rss_articles(self) -> list:
        print(f"ğŸ“¡ RSS í”¼ë“œ í™•ì¸: {PRONEWS_RSS}")
        feed = feedparser.parse(PRONEWS_RSS)
        articles = []
        for entry in feed.entries:
            if not FORCE_UPDATE and entry.link in self.posted_articles:
                continue
            try:
                article_date = datetime(*entry.published_parsed[:6])
            except:
                article_date = datetime.now()
            articles.append({
                'title': entry.title,
                'link': entry.link,
                'date': article_date,
                'source': 'rss'
            })
        print(f"   RSS ë¯¸ê²Œì‹œ: {len(articles)}ê±´")
        return articles

    def fetch_archive_articles(self, need: int, oldest_first: bool = False) -> list:
        print(f"ğŸ“š ì•„ì¹´ì´ë¸Œ í¬ë¡¤ë§ (í•„ìš”: {need}ê±´, ì˜¤ë˜ëœìˆœ: {oldest_first})...")
        collected = []
        seen_links = set()
        page = 1 if not oldest_first else ARCHIVE_MAX_PAGES

        while len(collected) < need * 3 and 1 <= page <= ARCHIVE_MAX_PAGES:
            url = f"{PRONEWS_ARCHIVE_BASE}/{page}/"
            try:
                res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=15)
                if res.status_code == 404:
                    print(f"   í˜ì´ì§€ {page} ì—†ìŒ â†’ ì¢…ë£Œ")
                    break
                res.raise_for_status()
                soup = BeautifulSoup(res.text, 'lxml')
                found = []

                for article in soup.find_all('article'):
                    a_tag = article.find('a', href=True)
                    if not a_tag:
                        continue
                    link = a_tag['href']
                    if not link.startswith('http'):
                        link = urljoin("[https://jp.pronews.com](https://jp.pronews.com)", link)
                    if '/news/' not in link or link in seen_links:
                        continue

                    title_tag = article.find(['h2', 'h3', 'h1'])
                    title = title_tag.get_text(strip=True) if title_tag else a_tag.get_text(strip=True)
                    if not title:
                        continue

                    date_tag = article.find('time')
                    article_date = datetime.now()
                    if date_tag:
                        try:
                            article_date = datetime.fromisoformat(
                                date_tag.get('datetime', date_tag.get_text(strip=True))[:19]
                            )
                        except:
                            pass

                    found.append({'title': title, 'link': link, 'date': article_date, 'source': 'archive'})

                if not found:
                    for a in soup.find_all('a', href=True):
                        href = a['href']
                        if not href.startswith('http'):
                            href = urljoin("[https://jp.pronews.com](https://jp.pronews.com)", href)
                        if re.search(r'/news/\d{10,}', href) and href not in seen_links:
                            title = a.get_text(strip=True)
                            if title and len(title) > 5:
                                found.append({'title': title, 'link': href,
                                              'date': datetime.now(), 'source': 'archive'})

                for art in found:
                    if art['link'] not in seen_links:
                        seen_links.add(art['link'])
                        if FORCE_UPDATE or art['link'] not in self.posted_articles:
                            collected.append(art)

                print(f"   í˜ì´ì§€ {page}: {len(found)}ê±´ ë°œê²¬, ëˆ„ì  ë¯¸ê²Œì‹œ: {len(collected)}ê±´")
                page = page + 1 if not oldest_first else page - 1
                time.sleep(1)

            except Exception as e:
                print(f"âš ï¸ ì•„ì¹´ì´ë¸Œ í˜ì´ì§€ {page} ì˜¤ë¥˜: {e}")
                page = page + 1 if not oldest_first else page - 1

        collected.sort(key=lambda x: x['date'], reverse=not oldest_first)
        result = collected[:need]
        print(f"   ì•„ì¹´ì´ë¸Œ ìˆ˜ì§‘ ì™„ë£Œ: {len(result)}ê±´")
        return result

    def get_articles_to_process(self) -> list:
        if IS_SCHEDULED:
            print("ğŸ• ìë™ ì‹¤í–‰: ìµœì‹  ìš°ì„  + ì•„ì¹´ì´ë¸Œ ë³´ì¶©")
            rss = self.fetch_rss_articles()
            rss.sort(key=lambda x: x['date'], reverse=True)
            target = rss[:DAILY_LIMIT]
            need = DAILY_LIMIT - len(target)
            if need > 0:
                print(f"   RSS {len(target)}ê±´ â†’ ì•„ì¹´ì´ë¸Œì—ì„œ {need}ê±´ ë³´ì¶©")
                rss_links = {a['link'] for a in target}
                archive = self.fetch_archive_articles(need * 2, oldest_first=False)
                archive = [a for a in archive if a['link'] not in rss_links]
                target += archive[:need]
            target = target[:DAILY_LIMIT]
        else:
            print("ğŸ“– ìˆ˜ë™ ì‹¤í–‰: ì•„ì¹´ì´ë¸Œ ì˜¤ë˜ëœ ìˆœ 10ê±´ (ë¸”ë¡œê·¸ ì±„ìš°ê¸°)")
            target = self.fetch_archive_articles(DAILY_LIMIT, oldest_first=True)

        print(f"âœ… ì²˜ë¦¬ ëŒ€ìƒ: {len(target)}ê±´")
        return target

    def fetch_full_content(self, url: str) -> str:
        try:
            print(f"ğŸ“„ ìŠ¤í¬ë˜í•‘: {url}")
            res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}, timeout=15)
            res.raise_for_status()
            soup = BeautifulSoup(res.text, 'lxml')

            # 1. ë³¸ë¬¸ ì˜ì—­ì„ ë” ì •ë°€í•˜ê²Œ ì°¾ê¸°
            content_div = (
                soup.find('div', class_='articleBody-inner') or
                soup.find('div', class_='articleBody') or
                soup.find('div', class_='entry-content') or
                soup.find('div', class_='post-content') or
                soup.find('div', class_='article-content') or
                soup.find('article')
            )
            if not content_div:
                return ""

            # =========================================================
            # [ì¶”ê°€] 2. ì§€ì €ë¶„í•œ ì›¹ì‚¬ì´íŠ¸ ê»ë°ê¸°(UI, ê´€ë ¨ê¸°ì‚¬, ë©”ë‰´) ê°•ì œ ì‚­ì œ
            noise_classes = [
                'articleAside', 'mainLayout-side', 'articleShareSticky', 
                'articleShare', 'relatedKeyword', 'relatedArticle', 'prnbox'
            ]
            for noise_class in noise_classes:
                for noise in content_div.find_all(class_=noise_class):
                    noise.decompose()
            # =========================================================

            removed = False
            for mv_class in ['articleBody-mv', 'article-mv', 'post-thumbnail',
                             'entry-thumbnail', 'article-eye-catch']:
                mv_area = content_div.find(class_=mv_class)
                if mv_area:
                    mv_area.decompose()
                    print(f"ğŸ—‘ï¸ ë³¸ë¬¸ ìƒë‹¨ ì´ë¯¸ì§€ ì œê±° ({mv_class})")
                    removed = True
                    break

            if not removed:
                first_child = content_div.find(recursive=False)
                if first_child and first_child.name in ['figure', 'picture']:
                    first_child.decompose()
                    print("ğŸ—‘ï¸ ë³¸ë¬¸ ìµœìƒë‹¨ figure ì œê±°")
                elif first_child and first_child.name == 'img':
                    first_child.decompose()
                    print("ğŸ—‘ï¸ ë³¸ë¬¸ ìµœìƒë‹¨ img ì œê±°")
                elif first_child and first_child.name in ['div', 'p']:
                    inner = first_child.find_all(recursive=False)
                    if len(inner) == 1 and inner[0].name in ['img', 'figure', 'picture']:
                        first_child.decompose()
                        print("ğŸ—‘ï¸ ë³¸ë¬¸ ìµœìƒë‹¨ ì´ë¯¸ì§€ ë˜í¼ ì œê±°")

            for elem in content_div.find_all(string=re.compile(
                r'åŸæ–‡æ²è¼‰æ™‚åˆ»:|ã‚½ãƒ¼ã‚¹:|ãƒãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼|é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰|ã“ã®è¨˜äº‹ã‚’ã‚·ã‚§ã‚¢|FOLLOW US'
            )):
                parent = elem.find_parent()
                if parent:
                    parent.decompose()

            for h_tag in content_div.find_all(['h2', 'h3', 'h4']):
                if any(kw in h_tag.get_text(strip=True) for kw in
                       ['ãƒãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼', 'ã“ã®è¨˜äº‹ã‚’ã‚·ã‚§ã‚¢', 'FOLLOW US', 'é–¢é€£è¨˜äº‹', 'é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰']):
                    next_elem = h_tag.find_next_sibling()
                    h_tag.decompose()
                    while next_elem and next_elem.name not in ['h1', 'h2', 'h3', 'h4']:
                        temp = next_elem.find_next_sibling()
                        next_elem.decompose()
                        next_elem = temp

            for tag in content_div(['script', 'style', 'noscript', 'form', 'nav', 'aside', 'footer', 'header']):
                tag.decompose()

            for iframe in list(content_div.find_all('iframe')):
                if not any(v in iframe.get('src', '').lower() for v in ['youtube', 'youtu.be', 'vimeo']):
                    iframe.decompose()

            for elem in content_div.find_all(class_=lambda x: x and any(
                sc in ' '.join(x).lower() for sc in
                ['social-share', 'share-buttons', 'addtoany', 'sharedaddy', 'entry-footer', 'post-meta']
            )):
                elem.decompose()

            for a in list(content_div.find_all('a')):
                href = a.get('href', '')
                if any(kw in href.lower() for kw in
                       ['facebook.com', 'twitter.com', 'line.me', '/fellowship/', 'hatena.ne.jp']) \
                        or href.startswith('//') or not a.get_text(strip=True):
                    a.decompose()

            for tag_name in ['p', 'div', 'span', 'li']:
                for tag in content_div.find_all(tag_name):
                    if not tag.get_text(strip=True) and not tag.find('img'):
                        tag.decompose()

            # HTML êµ¬ì¡° ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ë„ë¡ ë³€ê²½ (ê¸°ì¡´ get_text ì‚­ì œ)
            return str(content_div)

        except Exception as e:
            print(f"âš ï¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return ""

    def generate_seo_slug(self, title_ko: str, article_date: datetime) -> str:
        slug = re.sub(r'[^a-zA-Z0-9\s]', '', title_ko)
        slug = slug.lower().strip().replace(' ', '-')
        slug = re.sub(r'-+', '-', slug).strip('-')
        date_str = article_date.strftime('%Y%m%d')
        return f"{slug[:50]}-{date_str}" if len(slug) >= 3 else f"news-{date_str}"

    def get_main_image_url(self, link: str):
        try:
            res = requests.get(link, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
            soup = BeautifulSoup(res.text, 'lxml')
            og = soup.find('meta', property='og:image')
            if og and og.get('content'):
                return og['content']
            content = soup.find('div', class_='entry-content')
            if content:
                img = content.find('img')
                if img and img.get('src'):
                    src = img['src']
                    return src if src.startswith('http') else urljoin(link, src)
        except:
            pass
        return None

    def download_image(self, url: str):
        if not url:
            return None
        try:
            print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ: {url}")
            res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=15)
            res.raise_for_status()
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
            ext = os.path.splitext(os.path.basename(urlparse(url).path).split('?')[0])[1]
            if ext not in ['.jpg', '.jpeg', '.png', '.gif', '.webp']:
                ext = '.jpg'
            path = Path(f"/tmp/pronews_{int(time.time())}_{url_hash}{ext}")
            with open(path, 'wb') as f:
                f.write(res.content)
            print(f"   âœ… {path.name}")
            return path
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def upload_media(self, image_path: Path):
        if not image_path or not image_path.exists():
            return None
        try:
            with open(image_path, 'rb') as img:
                res = requests.post(
                    f"{self.wordpress_api}/media",
                    auth=(WORDPRESS_USER, WORDPRESS_APP_PASSWORD),
                    headers={'Content-Disposition': f'attachment; filename={image_path.name}'},
                    files={'file': (image_path.name, img, 'image/jpeg')}
                )
                res.raise_for_status()
                return res.json()
        except Exception as e:
            print(f"âš ï¸ ë¯¸ë””ì–´ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def is_already_posted_on_wp(self, original_url: str) -> bool:
        try:
            search_term = original_url.split('/')[-2] if original_url.endswith('/') else original_url.split('/')[-1]
            res = requests.get(
                f"{self.wordpress_api}/posts",
                auth=(WORDPRESS_USER, WORDPRESS_APP_PASSWORD),
                params={'search': search_term, 'per_page': 5, 'status': 'any'},
                timeout=10
            )
            if res.status_code == 200:
                for post in res.json():
                    if original_url in post.get('content', {}).get('rendered', ''):
                        print(f"âš ï¸ ì¤‘ë³µ ê°ì§€ â†’ ìŠ¤í‚µ: {post['link']}")
                        return True
            return False
        except Exception as e:
            print(f"âš ï¸ ì¤‘ë³µ ì²´í¬ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {e}")
            return False

    def commit_posted_articles(self):
        try:
            import subprocess
            subprocess.run(['git', 'config', 'user.email', 'action@github.com'], check=True)
            subprocess.run(['git', 'config', 'user.name', 'GitHub Action'], check=True)
            subprocess.run(['git', 'add', POSTED_ARTICLES_FILE], check=True)
            result = subprocess.run(['git', 'diff', '--cached', '--quiet'], capture_output=True)
            if result.returncode != 0:
                subprocess.run(['git', 'commit', '-m',
                    f'chore: update posted_articles [{datetime.now().strftime("%Y-%m-%d %H:%M")}]'], check=True)
                subprocess.run(['git', 'push'], check=True)
                print("ğŸ“ posted_articles.json â†’ git ì»¤ë°‹ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ git ì»¤ë°‹ ì‹¤íŒ¨: {e}")

    def post_to_wordpress(self, title: str, content: str, slug: str,
                           featured_media_id: int, original_date: datetime,
                           excerpt: str = "", status: str = "draft") -> bool:
        post_data = {
            'title': title, 'content': content, 'slug': slug,
            'status': status, 'featured_media': featured_media_id or 0,
            'date': original_date.strftime('%Y-%m-%dT%H:%M:%S')
        }
        if excerpt:
            post_data['excerpt'] = excerpt
        try:
            res = requests.post(
                f"{self.wordpress_api}/posts",
                auth=(WORDPRESS_USER, WORDPRESS_APP_PASSWORD),
                json=post_data
            )
            res.raise_for_status()
            label = "ğŸ“ ì„ì‹œì €ì¥" if status == "draft" else "âœ¨ ê²Œì‹œ ì„±ê³µ"
            print(f"{label}: {res.json()['link']}")
            return True
        except Exception as e:
            print(f"âŒ ê²Œì‹œ ì‹¤íŒ¨: {e}")
            if hasattr(e, 'response') and e.response is not None:
                print(f"   {e.response.text[:300]}")
            return False

    def process_article(self, article: dict) -> bool:
        print(f"\n{'='*60}")
        print(f"ğŸ“° {article['title'][:70]}")
        print(f"ğŸ“… {article['date'].strftime('%Y-%m-%d %H:%M')} [{article.get('source','?')}]")
        print(f"{'='*60}")

        if self.gemini.rate_limit_hit:
            print("ğŸ›‘ 429 í”Œë˜ê·¸ â†’ ë‹¤ìŒ ëŸ° ì´ì›”")
            return False

        if not FORCE_UPDATE and self.is_already_posted_on_wp(article['link']):
            if article['link'] not in self.posted_articles:
                self.posted_articles.append(article['link'])
                self.save_posted_articles()
            return False

        body_text = self.fetch_full_content(article['link'])
        if not body_text:
            print("âš ï¸ ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ â†’ ìŠ¤í‚µ")
            return False

        print("ğŸ”„ [1ë‹¨ê³„] Gemini ë²ˆì—­ (1íšŒ JSON í†µí•©)...")
        translated = self.gemini.translate_article(article['title'], body_text)

        if not translated or not translated.get('title') or not translated.get('content'):
            print("âŒ ë²ˆì—­ ì‹¤íŒ¨ â†’ ìŠ¤í‚µ")
            return False

        title_ko  = translated['title']
        content_ko = translated['content']
        excerpt   = translated.get('excerpt', '')
        tldr_html = translated.get('tldr', '')
        print(f"   ğŸ“Œ ì œëª©: {title_ko}")

        if self.gemini._has_japanese(content_ko):
            print("   âš ï¸ ì¼ë³¸ì–´ ì”ì¡´ â†’ ì¬ë²ˆì—­ 1íšŒ ì‹œë„...")
            content_ko = self.gemini.retranslate_content(content_ko)
            if self.gemini._has_japanese(content_ko):
                print("   âš ï¸ ì¬ë²ˆì—­ í›„ ì¼ë¶€ ì”ì¡´ â†’ ê²½ê³  í›„ ê²Œì‹œ ì§„í–‰")

        slug = self.generate_seo_slug(title_ko, article['date'])
        print(f"ğŸ”— Slug: {slug}")

        print("ğŸ” íŠ¹ì„± ì´ë¯¸ì§€(Featured Image) ì²˜ë¦¬ ì¤‘...")
        featured_id = 0
        img_url = self.get_main_image_url(article['link'])
        if img_url:
            local_img = self.download_image(img_url)
            if local_img:
                media_info = self.upload_media(local_img)
                if media_info:
                    featured_id = media_info['id']
                try:
                    local_img.unlink()
                except:
                    pass

        final_content = ""
        if tldr_html:
            final_content += (
                '<div style="background:#f8f9fa;padding:20px;border-radius:8px;'
                'border-left:5px solid #0056b3;margin-bottom:30px;">\n'
                '<h3 style="margin-top:0;color:#0056b3;">ğŸ’¡ í•µì‹¬ ìš”ì•½</h3>\n'
                f'{tldr_html}\n</div>\n\n'
            )
        final_content += content_ko
        final_content += (
            "\n\n<hr style='margin:40px 0 20px 0;border:0;border-top:1px solid #e0e0e0;'>\n"
            f"<p style='font-size:13px;color:#777;'><strong>ì›ë¬¸:</strong> "
            f"<a href='{article['link']}' target='_blank' rel='noopener'>{article['title']}</a></p>"
        )

        label = "draft(ì„ì‹œì €ì¥)" if POST_STATUS == "draft" else "publish(ì¦‰ì‹œê³µê°œ)"
        print(f"ğŸ“¤ [2ë‹¨ê³„] WordPress {label} ì¤‘...")
        if self.post_to_wordpress(title_ko, final_content, slug, featured_id,
                                   article['date'], excerpt=excerpt, status=POST_STATUS):
            if not FORCE_UPDATE:
                self.posted_articles.append(article['link'])
                self.save_posted_articles()
            return True
        return False

    def run(self):
        print(f"\n{'='*60}")
        print(f"pronews.jp â†’ prodg.kr ìë™ ë²ˆì—­ v7.2")
        print(f"ì—”ì§„: {GEMINI_MODEL} | í˜¸ì¶œ: ê¸°ì‚¬ë‹¹ 1íšŒ JSON í†µí•©")
        print(f"ëª¨ë“œ: {'ìë™ (ìµœì‹ â†’ì•„ì¹´ì´ë¸Œ ë³´ì¶©)' if IS_SCHEDULED else 'ìˆ˜ë™ (ì•„ì¹´ì´ë¸Œ ì˜¤ë˜ëœ ìˆœ)'}")
        print(f"ê²Œì‹œ: {POST_STATUS.upper()} | ì¼ì¼ í•œë„: {DAILY_LIMIT}ê±´")
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*60}\n")

        if not WORDPRESS_USER or not WORDPRESS_APP_PASSWORD:
            print("âŒ WP_USER / WP_APP_PASSWORD í™˜ê²½ë³€ìˆ˜ í•„ìš”")
            sys.exit(1)

        print("ğŸ”‘ Gemini API í‚¤ ê²€ì¦...")
        test = self.gemini._call_api("ãƒ†ã‚¹ãƒˆë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­:", max_tokens=30)
        if not test:
            print("âŒ Gemini API í‚¤ ì˜¤ë¥˜ â†’ ì¢…ë£Œ")
            sys.exit(1)
        print(f"   âœ… API ì •ìƒ: '{test}'")

        articles = self.get_articles_to_process()
        if not articles:
            print("âœ… ì²˜ë¦¬í•  ê¸°ì‚¬ ì—†ìŒ")
            return

        success = 0
        try:
            for i, article in enumerate(articles, 1):
                if self.gemini.rate_limit_hit:
                    print(f"\nğŸ›‘ 429 ëŸ° ì¢…ë£Œ â†’ ë‚¨ì€ {len(articles)-i+1}ê±´ ë‹¤ìŒ ëŸ° ì´ì›”")
                    break
                print(f"\n[{i}/{len(articles)}]")
                if self.process_article(article):
                    success += 1
                if i < len(articles):
                    time.sleep(10)
        finally:
            print(f"\n{'='*60}")
            print(f"ğŸ ì™„ë£Œ: {success}/{len(articles)}ê±´ ê²Œì‹œ")
            print(f"{'='*60}\n")
            if success > 0:
                self.commit_posted_articles()


if __name__ == "__main__":
    bot = NewsTranslator()
    bot.run()
