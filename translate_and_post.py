#!/usr/bin/env python3
"""
pronews.jp ìë™ ë²ˆì—­ ì‹œìŠ¤í…œ v3 (ìµœì¢…)
ê°œì„ ì‚¬í•­:
1. ìµœì‹  ê¸°ì‚¬ë¶€í„° 10ê±´ì”© ë²ˆì—­ (ì˜¤ë˜ëœ ê¸°ì‚¬ëŠ” ë‚˜ì¤‘ì—)
2. ì›ë¬¸ ê²Œì‹œì‹œê°, ì¶œì²˜ í…ìŠ¤íŠ¸ ì œê±°
3. ì˜ë¬¸ slug + ë¶ˆí•„ìš” ì½˜í…ì¸  ì œê±°
"""

import os
import sys
import requests
import feedparser
from datetime import datetime, timedelta
from pathlib import Path
import json
import time
from urllib.parse import urlparse, urljoin
from googletrans import Translator
import html2text
from bs4 import BeautifulSoup
import hashlib
import re

# ==========================================
# ì„¤ì •
# ==========================================
WORDPRESS_URL = "https://prodg.kr"
WORDPRESS_USER = os.environ.get("WP_USER")
WORDPRESS_APP_PASSWORD = os.environ.get("WP_APP_PASSWORD")
PRONEWS_RSS = "https://jp.pronews.com/feed"
POSTED_ARTICLES_FILE = "posted_articles.json"
FORCE_UPDATE = os.environ.get("FORCE_UPDATE", "false").lower() == "true"

class NewsTranslator:
    def __init__(self):
        self.translator = Translator()
        self.wordpress_api = f"{WORDPRESS_URL}/wp-json/wp/v2"
        self.posted_articles = self.load_posted_articles()
        
    def load_posted_articles(self):
        if Path(POSTED_ARTICLES_FILE).exists():
            with open(POSTED_ARTICLES_FILE, 'r') as f:
                try:
                    return json.load(f)
                except:
                    return []
        return []
        
    def save_posted_articles(self):
        with open(POSTED_ARTICLES_FILE, 'w') as f:
            json.dump(self.posted_articles, f, indent=2)
        
    def fetch_rss_feed(self):
        """
        [ê°œì„  1] ìµœì‹  ê¸°ì‚¬ë¶€í„° ëª¨ë‘ ì²˜ë¦¬ (ì œí•œ ì—†ìŒ)
        """
        print(f"ğŸ“¡ RSS í”¼ë“œ í™•ì¸ ì¤‘: {PRONEWS_RSS}")
        feed = feedparser.parse(PRONEWS_RSS)
        
        all_articles = []
        print(f"ğŸ” ì´ {len(feed.entries)}ê°œì˜ í”¼ë“œ í•­ëª© ê²€ìƒ‰...")

        for entry in feed.entries:
            if not FORCE_UPDATE and entry.link in self.posted_articles:
                continue
                
            try:
                article_date = datetime(*entry.published_parsed[:6])
            except:
                article_date = datetime.now()
                
            all_articles.append({
                'title': entry.title,
                'link': entry.link,
                'date': article_date
            })
        
        # [ê°œì„  1] ìµœì‹ ìˆœ ì •ë ¬ (ì—­ìˆœ)
        all_articles.sort(key=lambda x: x['date'], reverse=True)
        
        print(f"âœ… ì²˜ë¦¬í•  ìµœì‹  ê¸°ì‚¬: {len(all_articles)}ê°œ (ì œí•œ ì—†ìŒ)")
        return all_articles  # ëª¨ë“  ê¸°ì‚¬ ë°˜í™˜
        
    def fetch_full_content(self, url):
        """
        [ê°œì„  2] ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘ + ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
        """
        try:
            print(f"ğŸ“„ ìŠ¤í¬ë˜í•‘: {url}")
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'lxml')
            
            # pronews.comì˜ ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸°
            content_div = soup.find('div', class_='entry-content')
            if not content_div:
                content_div = soup.find('div', class_='post-content')
            if not content_div:
                content_div = soup.find('div', class_='article-content')
            if not content_div:
                content_div = soup.find('article')
                
            if not content_div:
                return None

            # [ê°œì„  2] "ì›ë¬¸ ê²Œì‹œì‹œê°", "ì¶œì²˜" í…ìŠ¤íŠ¸ ì œê±°
            for elem in content_div.find_all(string=re.compile(r'ì›ë¬¸ ê²Œì‹œì‹œê°:|ì¶œì²˜:|åŸæ–‡æ²è¼‰æ™‚åˆ»:|ã‚½ãƒ¼ã‚¹:|ãƒãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼|é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰|ã“ã®è¨˜äº‹ã‚’ã‚·ã‚§ã‚¢|FOLLOW US')):
                parent = elem.find_parent()
                if parent:
                    # í•´ë‹¹ ë¬¸ë‹¨ ì „ì²´ ì œê±°
                    parent.decompose()
            
            # h3 ì œëª©ì´ "ë°± ë„˜ë²„", "ê´€ë ¨ í‚¤ì›Œë“œ", "ì´ ê¸°ì‚¬ ê³µìœ " ë“±ì¸ ì„¹ì…˜ ì œê±°
            for h_tag in content_div.find_all(['h3', 'h2', 'h4']):
                h_text = h_tag.get_text(strip=True)
                if any(keyword in h_text for keyword in ['ë°± ë„˜ë²„', 'é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'ãƒãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼', 
                                                          'ã“ã®è¨˜äº‹ã‚’ã‚·ã‚§ã‚¢', 'ì´ ê¸°ì‚¬ ê³µìœ ', 'FOLLOW US',
                                                          'é–¢é€£è¨˜äº‹', 'ê´€ë ¨ ê¸°ì‚¬']):
                    # h íƒœê·¸ ë‹¤ìŒì˜ ëª¨ë“  í˜•ì œ ìš”ì†Œë„ ì œê±° (ì„¹ì…˜ ì „ì²´)
                    next_elem = h_tag.find_next_sibling()
                    h_tag.decompose()
                    while next_elem and next_elem.name not in ['h1', 'h2', 'h3', 'h4']:
                        temp = next_elem.find_next_sibling()
                        next_elem.decompose()
                        next_elem = temp

            # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì™„ì „ ì œê±°
            for tag in content_div(['script', 'style', 'iframe', 'noscript', 'form', 
                                   'nav', 'aside', 'footer', 'header']):
                tag.decompose()
            
            # ì†Œì…œ ê³µìœ  ë²„íŠ¼ ì œê±° (í´ë˜ìŠ¤ëª… ê¸°ë°˜)
            for social_class in ['social-share', 'share-buttons', 'sns-share', 'social-links', 
                                'share-links', 'addtoany', 'sharedaddy', 'jp-relatedposts',
                                'entry-footer', 'post-tags', 'post-categories', 'post-meta']:
                for elem in content_div.find_all(class_=lambda x: x and any(sc in str(x).lower() for sc in [social_class])):
                    elem.decompose()
            
            # íŠ¹ì • í…ìŠ¤íŠ¸ í¬í•¨ ìš”ì†Œ ì œê±°
            remove_keywords = [
                'FOLLOW US', 'ê´€ë ¨ ê¸°ì‚¬', 'Related', 'Share this', 'Tweet',
                'ë‰´ìŠ¤ ì¼ëŒ', 'ì¹¼ëŸ¼ íƒ€ì´í‹€', 'íŠ¹ì§‘ íƒ€ì´í‹€', 'ë¼ì´í„° ëª©ë¡',
                'facebook.com', 'twitter.com', 'line.me', 'instagram.com',
                'youtube.com', 'pronews.jp', 'kr.pronews.com', '/fellowship/',
                'getpocket.com', 'hatena.ne.jp', '/feed', '/news/', '/columntitle/',
                '/specialtitle/', '/writer/', 'jp.pronews.com'
            ]
            
            # a íƒœê·¸ ì œê±° (ë³¸ë¬¸ ì™¸ë¶€ ë§í¬)
            for a in list(content_div.find_all('a')):
                href = a.get('href', '')
                text = a.get_text(strip=True)
                
                # ì œê±° ì¡°ê±´
                should_remove = any([
                    any(kw in href.lower() for kw in remove_keywords),
                    any(kw in text for kw in ['FOLLOW', 'Share', 'Tweet', 'More', 'Read more']),
                    href.startswith('//www.facebook.com'),
                    href.startswith('//twitter.com'),
                    href.startswith('//line.me'),
                    href.startswith('//'),  # í”„ë¡œí† ì½œ ì—†ëŠ” ì™¸ë¶€ ë§í¬
                    not text  # ë¹ˆ ë§í¬
                ])
                
                if should_remove:
                    a.decompose()
            
            # ë¹ˆ íƒœê·¸ ì œê±°
            for tag_name in ['p', 'div', 'span', 'li', 'ul', 'ol']:
                for tag in content_div.find_all(tag_name):
                    if not tag.get_text(strip=True) and not tag.find('img'):
                        tag.decompose()
            
            # ì—°ì†ëœ br íƒœê·¸ ì •ë¦¬
            for br in content_div.find_all('br'):
                next_sibling = br.find_next_sibling()
                if next_sibling and next_sibling.name == 'br':
                    br.decompose()
                    
            return str(content_div)
            
        except Exception as e:
            print(f"âš ï¸ ì‹¤íŒ¨: {e}")
            return None

    def generate_english_slug(self, title):
        """ì˜ë¬¸ slug ìƒì„±"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì²« 3-5ë‹¨ì–´)
        words = title.split()[:5]
        
        # ì˜ë¬¸, ìˆ«ìë§Œ ì¶”ì¶œ
        slug_words = []
        for word in words:
            # ì˜ë¬¸ì, ìˆ«ì, í•˜ì´í”ˆë§Œ ë‚¨ê¹€
            cleaned = re.sub(r'[^a-zA-Z0-9\-]', '', word.lower())
            if cleaned and len(cleaned) > 2:
                slug_words.append(cleaned)
        
        # slug ìƒì„±
        if slug_words:
            slug = '-'.join(slug_words[:4])  # ìµœëŒ€ 4ë‹¨ì–´
        else:
            # ì˜ë¬¸ì´ ì—†ìœ¼ë©´ íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜
            slug = f"article-{int(time.time())}"
        
        # ê¸¸ì´ ì œí•œ (50ì)
        return slug[:50]

    def translate_text(self, text):
        """
        [ê°œì„  2] ë²ˆì—­ + "ì›ë¬¸ ê²Œì‹œì‹œê°", "ì¶œì²˜" ì œê±°
        [ê°œì„  4] HTML í—¤ë” íƒœê·¸ ìœ ì§€
        """
        if not text: 
            return ""
        
        try:
            # BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹±
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(text, 'lxml')
            
            # h1~h6 íƒœê·¸ë¥¼ ì„ì‹œë¡œ ì €ì¥
            headers = {}
            for i, tag in enumerate(soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])):
                placeholder = f"___HEADER_{i}___"
                headers[placeholder] = {
                    'tag': tag.name,
                    'class': tag.get('class', []),
                    'text': tag.get_text(strip=True)
                }
                tag.replace_with(placeholder)
            
            # HTMLì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            h = html2text.HTML2Text()
            h.ignore_links = False
            h.ignore_images = True
            h.body_width = 0
            plain_text = h.handle(str(soup))
            
            # [ê°œì„  2] "ì›ë¬¸ ê²Œì‹œì‹œê°:", "ì¶œì²˜:" í…ìŠ¤íŠ¸ ì œê±°
            plain_text = re.sub(r'ì›ë¬¸ ê²Œì‹œì‹œê°:.*?\n', '', plain_text)
            plain_text = re.sub(r'å‡ºå…¸:.*?\n', '', plain_text)
            plain_text = re.sub(r'ã‚½ãƒ¼ã‚¹:.*?\n', '', plain_text)
            plain_text = re.sub(r'åŸæ–‡æ²è¼‰æ™‚åˆ»:.*?\n', '', plain_text)
            
            # ë²ˆì—­
            if len(plain_text) > 4000:
                chunks = [plain_text[i:i+4000] for i in range(0, len(plain_text), 4000)]
                translated_parts = []
                for chunk in chunks:
                    res = self.translator.translate(chunk, src='ja', dest='ko')
                    translated_parts.append(res.text)
                    time.sleep(1)
                translated_text = "\n\n".join(translated_parts)
            else:
                result = self.translator.translate(plain_text, src='ja', dest='ko')
                time.sleep(0.5)
                translated_text = result.text
            
            # í—¤ë” íƒœê·¸ ë³µì›
            for placeholder, header_info in headers.items():
                tag_name = header_info['tag']
                classes = ' '.join(header_info['class']) if header_info['class'] else ''
                
                # í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì°¾ì•„ì„œ ë²ˆì—­
                if placeholder in translated_text:
                    # ì›ë³¸ í…ìŠ¤íŠ¸ë„ ë²ˆì—­
                    try:
                        translated_header = self.translator.translate(header_info['text'], src='ja', dest='ko').text
                        time.sleep(0.3)
                    except:
                        translated_header = header_info['text']
                    
                    # HTML íƒœê·¸ë¡œ ë³µì›
                    if classes:
                        replacement = f'<{tag_name} class="{classes}">{translated_header}</{tag_name}>'
                    else:
                        replacement = f'<{tag_name}>{translated_header}</{tag_name}>'
                    
                    translated_text = translated_text.replace(placeholder, replacement)
            
            return translated_text
            
        except Exception as e:
            print(f"âš ï¸ ë²ˆì—­ ì˜¤ë¥˜: {e}")
            return text

    def download_image(self, url):
        if not url: 
            return None
        try:
            print(f"ğŸ–¼ï¸ ë‹¤ìš´ë¡œë“œ: {url}")
            headers = {'User-Agent': 'Mozilla/5.0'}
            res = requests.get(url, headers=headers, timeout=15)
            res.raise_for_status()
            
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
            timestamp = int(time.time())
            
            original_filename = os.path.basename(urlparse(url).path)
            if '?' in original_filename:
                original_filename = original_filename.split('?')[0]
            
            ext = os.path.splitext(original_filename)[1]
            if not ext or ext not in ['.jpg', '.jpeg', '.png', '.gif', '.webp']:
                ext = '.jpg'
            
            filename = f"pronews_{timestamp}_{url_hash}{ext}"
            path = Path(f"/tmp/{filename}")
            
            with open(path, 'wb') as f:
                f.write(res.content)
            
            print(f"   âœ… {filename}")
            return path
        except Exception as e:
            print(f"âš ï¸ ì‹¤íŒ¨: {e}")
        return None

    def upload_media(self, image_path):
        if not image_path or not image_path.exists(): 
            return None
        try:
            url = f"{self.wordpress_api}/media"
            with open(image_path, 'rb') as img:
                files = {'file': (image_path.name, img, 'image/jpeg')}
                headers = {'Content-Disposition': f'attachment; filename={image_path.name}'}
                res = requests.post(
                    url,
                    auth=(WORDPRESS_USER, WORDPRESS_APP_PASSWORD),
                    headers=headers,
                    files=files
                )
                res.raise_for_status()
                return res.json()
        except Exception as e:
            print(f"âš ï¸ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

    def get_main_image_url(self, link):
        try:
            res = requests.get(link, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
            soup = BeautifulSoup(res.text, 'lxml')
            
            og_img = soup.find('meta', property='og:image')
            if og_img and og_img.get('content'):
                return og_img['content']
            
            content = soup.find('div', class_='entry-content')
            if content:
                img = content.find('img')
                if img and img.get('src'):
                    img_url = img['src']
                    if not img_url.startswith('http'):
                        img_url = urljoin(link, img_url)
                    return img_url
        except:
            pass
        return None

    def post_to_wordpress(self, title, content, slug, featured_media_id, original_date):
        post_data = {
            'title': title,
            'content': content,
            'slug': slug,
            'status': 'publish',
            'featured_media': featured_media_id if featured_media_id else 0,
            'date': original_date.strftime('%Y-%m-%dT%H:%M:%S')
        }
        
        try:
            res = requests.post(
                f"{self.wordpress_api}/posts",
                auth=(WORDPRESS_USER, WORDPRESS_APP_PASSWORD),
                json=post_data
            )
            res.raise_for_status()
            post_info = res.json()
            print(f"âœ¨ ê²Œì‹œ ì„±ê³µ! {post_info['link']}")
            return True
        except Exception as e:
            print(f"âŒ ì‹¤íŒ¨: {e}")
            if hasattr(e, 'response'):
                print(f"   {e.response.text[:200]}")
            return False

    def process_article(self, article):
        print(f"\n{'='*60}")
        print(f"ğŸ“° {article['title'][:50]}...")
        print(f"ğŸ“… {article['date'].strftime('%Y-%m-%d %H:%M')}")
        print(f"{'='*60}")
        
        # ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘
        raw_html = self.fetch_full_content(article['link'])
        if not raw_html:
            return False
            
        # ë²ˆì—­
        print(f"ğŸ”„ ë²ˆì—­ ì¤‘...")
        title_ko = self.translate_text(article['title'])
        content_ko = self.translate_text(raw_html)
        
        # ì˜ë¬¸ slug ìƒì„±
        slug = self.generate_english_slug(article['title'])
        print(f"ğŸ”— Slug: {slug}")
        
        # ì´ë¯¸ì§€ ì²˜ë¦¬
        print(f"ğŸ” ì´ë¯¸ì§€...")
        img_url = self.get_main_image_url(article['link'])
        featured_id = 0
        
        if img_url:
            local_img = self.download_image(img_url)
            if local_img:
                media_info = self.upload_media(local_img)
                if media_info:
                    featured_id = media_info['id']
                try: 
                    local_img.unlink()
                except: 
                    pass

        # ë³¸ë¬¸ êµ¬ì„±
        final_content = content_ko.replace("\n", "<br>\n")
        
        # ì›ë¬¸ ë§í¬ (í•˜ë‹¨)
        final_content += f"\n\n<hr style='margin:40px 0 20px 0;border:0;border-top:1px solid #e0e0e0;'>\n"
        final_content += f"<p style='font-size:13px;color:#777;'>"
        final_content += f"<strong>ì›ë¬¸:</strong> <a href='{article['link']}' target='_blank' rel='noopener'>{article['title']}</a>"
        final_content += f"</p>"
        
        # ê²Œì‹œ
        print(f"ğŸ“¤ ê²Œì‹œ...")
        if self.post_to_wordpress(title_ko, final_content, slug, featured_id, article['date']):
            if not FORCE_UPDATE:
                self.posted_articles.append(article['link'])
                self.save_posted_articles()
            return True
        return False

    def run(self):
        print(f"\n{'='*60}")
        print(f"pronews.jp â†’ prodg.kr ìë™ ë²ˆì—­ v3")
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*60}\n")
        
        if not WORDPRESS_USER or not WORDPRESS_APP_PASSWORD:
            print("âŒ í™˜ê²½ ë³€ìˆ˜ í•„ìš”!")
            sys.exit(1)

        articles = self.fetch_rss_feed()
        
        if not articles:
            print("âœ… ì²˜ë¦¬í•  ê¸°ì‚¬ ì—†ìŒ")
            return
        
        success = 0
        for article in articles:
            if self.process_article(article):
                success += 1
            time.sleep(3)
            
        print(f"\n{'='*60}")
        print(f"ğŸ ì™„ë£Œ: {success}/{len(articles)}ê°œ ìµœì‹  ê¸°ì‚¬ ê²Œì‹œ")
        print(f"{'='*60}\n")

if __name__ == "__main__":
    bot = NewsTranslator()
    bot.run()
