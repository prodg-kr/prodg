#!/usr/bin/env python3
"""
pronews.jp ìë™ ë²ˆì—­ ì‹œìŠ¤í…œ v6
íŒŒì´í”„ë¼ì¸: ì¼ë³¸ì–´ ì›ë¬¸ â†’ Gemini ë²ˆì—­+SEOí¸ì§‘ í†µí•© â†’ WordPress ê²Œì‹œ

v5 â†’ v6 ë³€ê²½ì‚¬í•­:
- Groq ì œê±° â†’ Gemini ë‹¨ì¼ ì—”ì§„ìœ¼ë¡œ í†µí•©
  (Groq llama-3.3-70bì˜ ì¼ë³¸ì–´â†’í•œêµ­ì–´ í’ˆì§ˆ ë¬¸ì œ í•´ê²°)
- ë²ˆì—­+SEOí¸ì§‘ì„ ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ë¡œ ì²˜ë¦¬ (ë¬¸ë§¥ ì¼ê´€ì„± í–¥ìƒ)
- ì œëª© ì˜ë¦¼ ë¬¸ì œ ìˆ˜ì •: ì œí’ˆëª… í¬í•¨ ì‹œ 50ìê¹Œì§€ í—ˆìš©
- ì¼ë³¸ì–´ ì”ì¡´ ê°ì§€ í›„ ì¬ë²ˆì—­ ì•ˆì „ë§ ì¶”ê°€
- POST_STATUS: publish / draft ì„ íƒ ê°€ëŠ¥
- excerpt ìë™ ìƒì„±
- ì¤‘ë³µ ë°©ì§€: posted_articles.json + WordPress API 2ì¤‘ ì²´í¬
- ê²Œì‹œ í›„ posted_articles.json git ìë™ ì»¤ë°‹
"""

import os
import sys
import requests
import feedparser
from datetime import datetime
from pathlib import Path
import json
import time
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
import hashlib
import re

# ==========================================
# ì„¤ì •
# ==========================================
WORDPRESS_URL          = "https://prodg.kr"
WORDPRESS_USER         = os.environ.get("WP_USER")
WORDPRESS_APP_PASSWORD = os.environ.get("WP_APP_PASSWORD")
GEMINI_API_KEY         = os.environ.get("GEMINI_API_KEY")
PRONEWS_RSS            = "https://jp.pronews.com/feed"
POSTED_ARTICLES_FILE   = "posted_articles.json"
FORCE_UPDATE           = os.environ.get("FORCE_UPDATE", "false").lower() == "true"
DAILY_LIMIT            = 10  # í•˜ë£¨ ìµœëŒ€ ê²Œì‹œ ê±´ìˆ˜

# ê²Œì‹œ ìƒíƒœ: publish(ì¦‰ì‹œê³µê°œ) / draft(ì„ì‹œì €ì¥ í›„ ìˆ˜ë™ ê²€ìˆ˜)
POST_STATUS      = os.environ.get("POST_STATUS", "publish")
GENERATE_EXCERPT = True  # WordPress SEOìš© ìš”ì•½ë¬¸ ìë™ ìƒì„±

# ëª¨ë¸ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ë¡œ êµì²´ ê°€ëŠ¥)
GEMINI_MODEL = os.environ.get("GEMINI_MODEL", "gemini-2.5-flash")


# ==========================================
# Gemini í†µí•© ì—”ì§„ (ë²ˆì—­ + SEO í¸ì§‘)
# ==========================================
class GeminiEngine:
    """
    Gemini ë‹¨ì¼ ì—”ì§„ìœ¼ë¡œ ë²ˆì—­+SEOí¸ì§‘ í†µí•© ì²˜ë¦¬
    - ì¼ë³¸ì–´ â†’ í•œêµ­ì–´ ë²ˆì—­ (Groq ëŒ€ë¹„ í’ˆì§ˆ ëŒ€í­ í–¥ìƒ)
    - SEO ìµœì í™” ì œëª© ì¬ì‘ì„±
    - í•©ì‡¼ì²´(~í•©ë‹ˆë‹¤) ë¬¸ì²´ í†µì¼
    - ì „ë¬¸ìš©ì–´ ì •í™•ì„± ë³´ì •
    - excerpt ìƒì„±
    """

    def __init__(self):
        self.api_key = GEMINI_API_KEY
        if not self.api_key:
            print("âŒ GEMINI_API_KEY ë¯¸ì„¤ì •")
            sys.exit(1)

    def _call_api(self, prompt: str, max_tokens: int = 4096) -> str:
        url = (
            f"https://generativelanguage.googleapis.com/v1beta/models/"
            f"{GEMINI_MODEL}:generateContent?key={self.api_key}"
        )
        payload = {
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {
                "maxOutputTokens": max_tokens,
                "temperature": 0.4,
                "thinkingConfig": {
                    "thinkingBudget": 0
                }
            }
        }
        max_retries = 3
        for attempt in range(max_retries):
            try:
                res = requests.post(url, json=payload, timeout=90)
                res.raise_for_status()
                candidates = res.json().get("candidates", [])
                if candidates:
                    parts = candidates[0]["content"]["parts"]
                    # thinking ëª¨ë¸: thought íŒŒíŠ¸ë¥¼ ê±´ë„ˆë›°ê³  ì‹¤ì œ ì‘ë‹µ ì¶”ì¶œ
                    result_text = ""
                    for part in parts:
                        if not part.get("thought", False) and "text" in part:
                            result_text = part["text"]
                    # fallback: thought íŒŒíŠ¸ë§Œ ìˆëŠ” ê²½ìš° ë§ˆì§€ë§‰ text íŒŒíŠ¸ ì‚¬ìš©
                    if not result_text:
                        for part in reversed(parts):
                            if "text" in part:
                                result_text = part["text"]
                                break
                    if result_text:
                        return result_text.strip()
                print(f"âš ï¸ Gemini ì‘ë‹µì— candidates ì—†ìŒ (ì‹œë„ {attempt+1}/{max_retries})")
            except Exception as e:
                print(f"âš ï¸ Gemini API ì˜¤ë¥˜ (ì‹œë„ {attempt+1}/{max_retries}): {e}")
                if hasattr(e, 'response') and e.response is not None:
                    print(f"   HTTP {e.response.status_code}: {e.response.text[:300]}")
            if attempt < max_retries - 1:
                wait = 5 * (attempt + 1)
                print(f"   â³ {wait}ì´ˆ í›„ ì¬ì‹œë„...")
                time.sleep(wait)
        print("âŒ Gemini API í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨ (3íšŒ ì‹œë„)")
        return ""

    def translate_and_edit_title(self, title_ja: str) -> str:
        """
        ì œëª© ë²ˆì—­ + SEO í¸ì§‘ í†µí•©
        - ì œí’ˆëª…/ëª¨ë¸ëª… ì ˆëŒ€ ì˜ë¦¬ì§€ ì•Šë„ë¡ ë³´í˜¸
        - ì œí’ˆëª… í¬í•¨ ì‹œ 50ìê¹Œì§€ í—ˆìš©, ì¼ë°˜ ì œëª©ì€ 35ì ë‚´ì™¸
        - ìµœì†Œ 10ì ë¯¸ë§Œì´ë©´ ì¬ì‹œë„ (í’ˆì§ˆ ê²€ì¦)
        """
        MIN_TITLE_LENGTH = 10  # ì œëª© ìµœì†Œ ê¸€ì ìˆ˜

        prompt = f"""ë‹¹ì‹ ì€ ì˜ìƒ/ì¹´ë©”ë¼ ì „ë¬¸ ë¯¸ë””ì–´ì˜ SEO ì—ë””í„°ì…ë‹ˆë‹¤.

ì¼ë³¸ì–´ ì œëª©: {title_ja}

ìœ„ ì œëª©ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ê³  êµ¬ê¸€ SEOì— ìµœì í™”í•˜ì„¸ìš”.

ê·œì¹™:
1. Sony, Canon, Nikon, DJI, Blackmagic, Sigma, NIKKOR, LUMIX, FUJIFILM ë“± ë¸Œëœë“œëª…/ì œí’ˆëª…/ëª¨ë¸ëª…ì€ ì›ë¬¸ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³  ì ˆëŒ€ ìƒëµí•˜ì§€ ë§ˆì„¸ìš”
2. ëª¨ë¸ ë²ˆí˜¸(ì˜ˆ: NIKKOR Z 70-200mm f/2.8 VR S II)ê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì „ì²´ í¬í•¨
3. ì œí’ˆëª…ì— í¬í•¨ëœ íŠ¹ìˆ˜ë¬¸ì(|, /, -, ., mm, f/ ë“±)ë„ ì›ë¬¸ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš” (ì˜ˆ: "DG | Art", "f/1.2" ë“±)
4. ê²€ìƒ‰ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì•ìª½ì— ë°°ì¹˜
5. ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ (ì§ì—­ì²´, ì–´ìƒ‰í•œ ì¡°ì‚¬ ê¸ˆì§€)
6. ì œí’ˆëª… ì—†ëŠ” ê²½ìš° 35ì ë‚´ì™¸, ì œí’ˆëª… í¬í•¨ ì‹œ 50ìê¹Œì§€ í—ˆìš©
7. ì›ë¬¸ì˜ í•µì‹¬ ì •ë³´(ì œí’ˆëª…, ë°œí‘œ/ì¶œì‹œ, ì´ë²¤íŠ¸ëª… ë“±)ë¥¼ ì ˆëŒ€ ìƒëµí•˜ì§€ ë§ˆì„¸ìš”
8. ì œëª©ë§Œ ì¶œë ¥ (ì„¤ëª…, ë”°ì˜´í‘œ, ë²ˆí˜¸ ì—†ìŒ)"""

        # ìµœëŒ€ 3íšŒ ì‹œë„ (ì´ˆê¸° 1íšŒ + ì¬ì‹œë„ 2íšŒ)
        for attempt in range(3):
            result = self._call_api(prompt, max_tokens=200)
            if result:
                result = re.sub(r'^[\d\.\)\-\s"\'ã€Œã€ã€ã€‘]+', '', result).strip().strip('"\'ã€Œã€ã€ã€‘')
                if len(result) >= MIN_TITLE_LENGTH:
                    print(f"   ğŸ“Œ ë²ˆì—­ ì œëª©: {result}")
                    return result
                else:
                    print(f"   âš ï¸ ì œëª©ì´ ë„ˆë¬´ ì§§ìŒ ({len(result)}ì: '{result}') â€” ì¬ì‹œë„ {attempt+1}/3")
                    time.sleep(2)
            else:
                print(f"   âš ï¸ ì œëª© ë²ˆì—­ API ì‹¤íŒ¨ â€” ì¬ì‹œë„ {attempt+1}/3")
                time.sleep(2)

        # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ ì‹œ ë‹¨ìˆœ ë²ˆì—­ í”„ë¡¬í”„íŠ¸ë¡œ ìµœì¢… ì‹œë„
        print("   ğŸ”„ ë‹¨ìˆœ ë²ˆì—­ í”„ë¡¬í”„íŠ¸ë¡œ ìµœì¢… ì‹œë„...")
        fallback_prompt = f"""ë‹¤ìŒ ì¼ë³¸ì–´ ì œëª©ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”. 
ì œí’ˆëª…/ëª¨ë¸ëª…/ë¸Œëœë“œëª…ì€ ì›ë¬¸ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”.
ë²ˆì—­ëœ ì œëª©ë§Œ ì¶œë ¥í•˜ì„¸ìš”.

{title_ja}"""
        result = self._call_api(fallback_prompt, max_tokens=200)
        if result:
            result = re.sub(r'^[\d\.\)\-\s"\'ã€Œã€ã€ã€‘]+', '', result).strip().strip('"\'ã€Œã€ã€ã€‘')
            if len(result) >= MIN_TITLE_LENGTH:
                print(f"   ğŸ“Œ ë²ˆì—­ ì œëª© (fallback): {result}")
                return result
            print(f"   âŒ fallbackë„ ì§§ì€ ì œëª© ë°˜í™˜: '{result}'")

        print(f"âŒ ì œëª© ë²ˆì—­ ì‹¤íŒ¨ â€” ì¼ë³¸ì–´ ì›ë¬¸ ë°˜í™˜ ë°©ì§€")
        return ""

    def translate_and_edit_content(self, html_content: str) -> str:
        """
        ë³¸ë¬¸ ë²ˆì—­ + SEO í¸ì§‘ í†µí•© (Gemini ë‹¨ì¼ ì²˜ë¦¬)
        - HTML êµ¬ì¡° ì™„ì „ ë³´ì¡´ (img, iframe, video, strong, em, a ë“±)
        - ì¼ë³¸ì–´ â†’ í•œêµ­ì–´ ë²ˆì—­ (ë¬¸ë§¥ ì¼ê´€ì„± ìœ ì§€)
        - ì§ì—­ì²´ â†’ ìì—°ìŠ¤ëŸ¬ìš´ í•©ì‡¼ì²´
        - ì¼ë³¸ì–´ ì”ì¡´ ì‹œ ì¬ë²ˆì—­
        """
        if not html_content:
            return ""

        soup = BeautifulSoup(html_content, 'lxml')

        # 1. ë¯¸ë””ì–´ íƒœê·¸ë¥¼ í”Œë ˆì´ìŠ¤í™€ë”ë¡œ ë³´í˜¸ (img, iframe, video, figure, source)
        protected_media = {}
        media_counter = 0
        for tag in soup.find_all(['img', 'iframe', 'video', 'figure', 'source', 'picture']):
            placeholder = f"___MEDIA_{media_counter}___"
            protected_media[placeholder] = str(tag)
            tag.replace_with(placeholder)
            media_counter += 1
        if media_counter > 0:
            print(f"   ğŸ–¼ï¸ ë¯¸ë””ì–´ {media_counter}ê°œ ë³´í˜¸ (ì´ë¯¸ì§€/ë™ì˜ìƒ)")

        # 2. ë²ˆì—­ ëŒ€ìƒ block ìš”ì†Œ ìˆ˜ì§‘ (innerHTML ë³´ì¡´)
        block_tags = ['p', 'li', 'blockquote', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']
        blocks = soup.find_all(block_tags)

        if not blocks:
            # block ìš”ì†Œ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
            full_text = soup.get_text(separator='\n', strip=True)
            if not full_text:
                return html_content
            translated = self._translate_chunk(full_text)
            if not translated:
                return ""
            result_html = f"<p>{translated}</p>"
            for ph, original in protected_media.items():
                result_html = result_html.replace(ph, original)
            return result_html

        # 3. block ìš”ì†Œì˜ innerHTML ì¶”ì¶œ + ì²­í¬ ë¬¶ê¸°
        translatable_blocks = []  # (block_element, inner_html)
        for block in blocks:
            inner_html = block.decode_contents().strip()
            if not inner_html:
                continue
            # ë¯¸ë””ì–´ í”Œë ˆì´ìŠ¤í™€ë”ë§Œ ìˆëŠ” ë¸”ë¡ì€ ë²ˆì—­ ë¶ˆí•„ìš”
            text_only = re.sub(r'___MEDIA_\d+___', '', inner_html)
            text_only = re.sub(r'<[^>]+>', '', text_only).strip()
            if not text_only or len(text_only) < 3:
                continue
            translatable_blocks.append((block, inner_html))

        if not translatable_blocks:
            # ë²ˆì—­í•  í…ìŠ¤íŠ¸ ë¸”ë¡ì´ ì—†ìœ¼ë©´ ë¯¸ë””ì–´ë§Œ ë³µì› í›„ ë°˜í™˜
            result_html = str(soup)
            for ph, original in protected_media.items():
                result_html = result_html.replace(ph, original)
            return result_html

        # 4. ì²­í¬ ë‹¨ìœ„ ë²ˆì—­ (Gemini í† í° í•œë„ ê³ ë ¤, ì²­í¬ë‹¹ 3000ì)
        #    êµ¬ë¶„ìë¡œ ë¸”ë¡ì„ ë¬¶ì–´ ë³´ë‚´ê³ , ê²°ê³¼ë¥¼ ë‹¤ì‹œ ë¶„ë¦¬
        SEPARATOR = "\n<!--BLOCK_SEP-->\n"
        chunks = []  # [(start_idx, end_idx, combined_html)]
        current_chunk = []
        current_size = 0
        start_idx = 0

        for i, (block, inner_html) in enumerate(translatable_blocks):
            if current_size + len(inner_html) > 3000 and current_chunk:
                chunks.append((start_idx, i, SEPARATOR.join(current_chunk)))
                current_chunk = []
                current_size = 0
                start_idx = i
            current_chunk.append(inner_html)
            current_size += len(inner_html)

        if current_chunk:
            chunks.append((start_idx, len(translatable_blocks), SEPARATOR.join(current_chunk)))

        # 5. ì²­í¬ë³„ ë²ˆì—­ ìˆ˜í–‰
        translated_blocks = []
        for chunk_start, chunk_end, chunk_html in chunks:
            translated = self._translate_chunk(chunk_html)
            if not translated:
                print(f"   âš ï¸ ì²­í¬ ë²ˆì—­ ì‹¤íŒ¨ ({chunk_start}-{chunk_end})")
                # ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ìœ ì§€
                for j in range(chunk_start, chunk_end):
                    translated_blocks.append(translatable_blocks[j][1])
                continue
            # ë²ˆì—­ ê²°ê³¼ë¥¼ êµ¬ë¶„ìë¡œ ë¶„ë¦¬
            parts = translated.split('<!--BLOCK_SEP-->')
            expected_count = chunk_end - chunk_start
            if len(parts) == expected_count:
                translated_blocks.extend([p.strip() for p in parts])
            else:
                # êµ¬ë¶„ì ê°œìˆ˜ê°€ ë§ì§€ ì•Šìœ¼ë©´ ê· ë“± ë¶„ë°° ì‹œë„
                print(f"   âš ï¸ ë¸”ë¡ ìˆ˜ ë¶ˆì¼ì¹˜ (ì˜ˆìƒ: {expected_count}, ì‹¤ì œ: {len(parts)}) â€” ì „ì²´ ì ìš©")
                if len(parts) >= expected_count:
                    translated_blocks.extend([p.strip() for p in parts[:expected_count]])
                else:
                    # ë¶€ì¡±í•˜ë©´ ë§ˆì§€ë§‰ íŒŒíŠ¸ì— ë‚˜ë¨¸ì§€ í•©ì¹˜ê¸°
                    for p in parts:
                        translated_blocks.append(p.strip())
                    for _ in range(expected_count - len(parts)):
                        translated_blocks.append('')
            time.sleep(1)

        # ë²ˆì—­ ê²°ê³¼ ê²€ì¦
        non_empty = [b for b in translated_blocks if b.strip()]
        total_blocks = len(translatable_blocks)
        if len(non_empty) < total_blocks * 0.3:
            print(f"âŒ ë³¸ë¬¸ ë²ˆì—­ ì‹¤íŒ¨ â€” ë²ˆì—­ëœ ë¸”ë¡ {len(non_empty)}/{total_blocks}ê°œ")
            return ""

        # 6. ë²ˆì—­ëœ ë‚´ìš©ì„ ì›ë˜ block ìš”ì†Œì— ì‚½ì… (HTML êµ¬ì¡° ë³´ì¡´)
        for i, (block, _) in enumerate(translatable_blocks):
            if i < len(translated_blocks) and translated_blocks[i]:
                block.clear()
                new_content = BeautifulSoup(translated_blocks[i], 'html.parser')
                for child in list(new_content.children):
                    block.append(child)

        # 7. ê²°ê³¼ HTML ìƒì„±
        # body íƒœê·¸ ì•ˆì˜ ë‚´ìš©ë§Œ ì¶”ì¶œ (lxmlì´ ìë™ ì¶”ê°€í•˜ëŠ” html/body ì œê±°)
        body = soup.find('body')
        result_html = body.decode_contents() if body else str(soup)

        # 8. ë¯¸ë””ì–´ í”Œë ˆì´ìŠ¤í™€ë” â†’ ì›ë³¸ íƒœê·¸ ë³µì›
        for placeholder, original in protected_media.items():
            result_html = result_html.replace(placeholder, original)

        # 9. ì¼ë³¸ì–´ ì”ì¡´ ê²€ì‚¬ â†’ ì”ì¡´ ì‹œ ì¬ë²ˆì—­
        if self._has_japanese(result_html):
            print("   âš ï¸ ì¼ë³¸ì–´ ì”ì¡´ ê°ì§€ â†’ ì¬ë²ˆì—­ ì‹œë„...")
            result_html = self._cleanup_japanese(result_html)

        return result_html

    def _translate_chunk(self, html_text: str) -> str:
        """HTML í¬í•¨ í…ìŠ¤íŠ¸ ì²­í¬ ë²ˆì—­ + SEO í¸ì§‘ í†µí•© í”„ë¡¬í”„íŠ¸"""
        if not html_text.strip():
            return html_text

        prompt = f"""ë‹¹ì‹ ì€ ì˜ìƒ/ì¹´ë©”ë¼ ì „ë¬¸ ë¯¸ë””ì–´ì˜ í•œêµ­ì–´ ì—ë””í„°ì…ë‹ˆë‹¤.

ì•„ë˜ ì¼ë³¸ì–´ í…ìŠ¤íŠ¸(HTML í¬í•¨)ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ í¸ì§‘í•˜ì„¸ìš”.

ë²ˆì—­+í¸ì§‘ ê·œì¹™:
1. ì¼ë³¸ì–´ë¥¼ ì™„ì „íˆ í•œêµ­ì–´ë¡œ ë²ˆì—­ (íˆë¼ê°€ë‚˜Â·ê°€íƒ€ì¹´ë‚˜Â·í•œì ë‹¨ì–´ ì ˆëŒ€ ë‚¨ê¸°ì§€ ë§ ê²ƒ)
2. ë¬¸ì²´ëŠ” ë°˜ë“œì‹œ '~í•©ë‹ˆë‹¤', '~í–ˆìŠµë‹ˆë‹¤', '~ì…ë‹ˆë‹¤' í•©ì‡¼ì²´ë¡œ í†µì¼
   ('~í•œë‹¤', '~í–ˆë‹¤', '~ì´ë‹¤' í‰ì„œì²´ ì‚¬ìš© ê¸ˆì§€)
3. ì§ì—­ì²´, ì–´ìƒ‰í•œ ì¡°ì‚¬, ì¼ë³¸ì‹ í‘œí˜„ì„ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ìˆ˜ì •
4. ì˜ìƒ/ì¹´ë©”ë¼ ì „ë¬¸ìš©ì–´ ì •í™•íˆ í‘œê¸°:
   - ë¸Œëœë“œëª…: Sony, Canon, Nikon, DJI, Blackmagic, Sigma, DaVinci Resolve ë“± ì›ë¬¸ ìœ ì§€
   - í•´ìƒë„: 4K, 8K, Full HD / í”„ë ˆì„ë ˆì´íŠ¸: fps, 24p, 60p
   - ê¸°íƒ€: ì½”ë±, ë¹„íŠ¸ë ˆì´íŠ¸, ì¡°ë¦¬ê°œ, ì…”í„°ìŠ¤í”¼ë“œ, ë³´ì¼€, ì†ë–¨ë¦¼ë³´ì • ë“±
5. HTML íƒœê·¸ë¥¼ ë°˜ë“œì‹œ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”:
   - <strong>, <b> (ë³¼ë“œ), <em>, <i> (ì´íƒ¤ë¦­) íƒœê·¸ëŠ” ì›ë¬¸ê³¼ ë™ì¼í•˜ê²Œ ë³´ì¡´
   - <a href="..."> ë§í¬ íƒœê·¸ì˜ href ì†ì„±ê³¼ êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€
   - <!--BLOCK_SEP--> êµ¬ë¶„ìëŠ” ì ˆëŒ€ ë³€ê²½í•˜ê±°ë‚˜ ì‚­ì œí•˜ì§€ ë§ˆì„¸ìš”
6. ___MEDIA_0___ ê°™ì€ í”Œë ˆì´ìŠ¤í™€ë”ëŠ” ì ˆëŒ€ ë³€ê²½í•˜ì§€ ë§ ê²ƒ
7. ë²ˆì—­ëœ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥ (ì„¤ëª… ì—†ìŒ)

ì¼ë³¸ì–´ í…ìŠ¤íŠ¸:
{html_text}"""

        result = self._call_api(prompt, max_tokens=4096)
        if not result:
            print(f"âŒ ì²­í¬ ë²ˆì—­ ì‹¤íŒ¨ â€” ì›ë¬¸ ë°˜í™˜ ë°©ì§€ (ì›ë¬¸ ê¸¸ì´: {len(html_text)}ì)")
            return ""
        return result

    def _translate_single(self, text: str) -> str:
        """ë‹¨ì¼ ì§§ì€ í…ìŠ¤íŠ¸ ë²ˆì—­ (í—¤ë”ìš©)"""
        if not text.strip():
            return text
        prompt = f"ë‹¤ìŒ ì¼ë³¸ì–´ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ í•©ì‡¼ì²´ë¡œ ë²ˆì—­í•˜ì„¸ìš”. ë²ˆì—­ë¬¸ë§Œ ì¶œë ¥:\n{text}"
        result = self._call_api(prompt, max_tokens=200)
        return result if result else text

    def _has_japanese(self, text: str) -> bool:
        """ì¼ë³¸ì–´(íˆë¼ê°€ë‚˜Â·ê°€íƒ€ì¹´ë‚˜) ì”ì¡´ ì—¬ë¶€ ê²€ì‚¬"""
        japanese_pattern = re.compile(r'[\u3040-\u309f\u30a0-\u30ff]')
        plain_text = BeautifulSoup(text, 'lxml').get_text()
        matches = japanese_pattern.findall(plain_text)
        return len(matches) > 5  # 5ì ì´ìƒ ì¼ë³¸ì–´ ì”ì¡´ ì‹œ ì¬ë²ˆì—­

    def _cleanup_japanese(self, html: str) -> str:
        """ì¼ë³¸ì–´ ì”ì¡´ ë¶€ë¶„ë§Œ ì¬ë²ˆì—­"""
        soup = BeautifulSoup(html, 'lxml')
        for elem in soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'li']):
            text = elem.get_text()
            if re.search(r'[\u3040-\u309f\u30a0-\u30ff]', text):
                translated = self._translate_single(text)
                if translated:
                    elem.string = translated
        return str(soup.find('body') or soup)

    def generate_excerpt(self, title_ko: str, content_ko: str) -> str:
        """
        WordPress SEOìš© ìš”ì•½ë¬¸(excerpt) ìƒì„±
        - 80ì ë‚´ì™¸, ê²€ìƒ‰ê²°ê³¼ ìŠ¤ë‹ˆí«ì— ìµœì í™”
        """
        soup = BeautifulSoup(content_ko, 'lxml')
        plain_text = soup.get_text(separator=' ', strip=True)[:500]

        prompt = f"""ë‹¹ì‹ ì€ SEO ì „ë¬¸ ì—ë””í„°ì…ë‹ˆë‹¤.

ê¸°ì‚¬ ì œëª©: {title_ko}
ë³¸ë¬¸ ì¼ë¶€: {plain_text}

êµ¬ê¸€ ê²€ìƒ‰ê²°ê³¼ì— ë…¸ì¶œë  ìš”ì•½ë¬¸(ë©”íƒ€ ë””ìŠ¤í¬ë¦½ì…˜)ì„ ì‘ì„±í•˜ì„¸ìš”.

ê·œì¹™:
1. 80ì ë‚´ì™¸ (ìµœëŒ€ 100ì)
2. í•µì‹¬ í‚¤ì›Œë“œ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨
3. ë…ìê°€ í´ë¦­í•˜ê³  ì‹¶ì–´ì§€ëŠ” ë¬¸ì¥
4. ~í•©ë‹ˆë‹¤ í•©ì‡¼ì²´ë¡œ ì‘ì„±
5. ìš”ì•½ë¬¸ë§Œ ì¶œë ¥ (ì„¤ëª… ì—†ìŒ)"""

        result = self._call_api(prompt, max_tokens=150)
        if result:
            result = result.strip().strip('"\'')
            print(f"   ğŸ“‹ ìš”ì•½ë¬¸: {result[:60]}...")
            return result
        return ""


# ==========================================
# ë©”ì¸ ë²ˆì—­ ì‹œìŠ¤í…œ
# ==========================================
class NewsTranslator:
    def __init__(self):
        self.gemini = GeminiEngine()
        self.wordpress_api = f"{WORDPRESS_URL}/wp-json/wp/v2"
        self.posted_articles = self.load_posted_articles()

    def load_posted_articles(self) -> list:
        if Path(POSTED_ARTICLES_FILE).exists():
            with open(POSTED_ARTICLES_FILE, 'r') as f:
                try:
                    return json.load(f)
                except:
                    return []
        return []

    def save_posted_articles(self):
        with open(POSTED_ARTICLES_FILE, 'w') as f:
            json.dump(self.posted_articles, f, indent=2)

    def fetch_rss_feed(self) -> list:
        """
        RSS í”¼ë“œì—ì„œ ë¯¸ê²Œì‹œ ê¸°ì‚¬ ì¡°íšŒ
        - ìµœì‹ ìˆœ ì •ë ¬
        - ìµœì‹  ê¸°ì‚¬ ë¶€ì¡± ì‹œ ê³¼ê±° ë¯¸ê²Œì‹œ ê¸°ì‚¬ë¡œ ì±„ì›Œ ìµœëŒ€ 10ê±´ ë°˜í™˜
        """
        print(f"ğŸ“¡ RSS í”¼ë“œ í™•ì¸ ì¤‘: {PRONEWS_RSS}")
        feed = feedparser.parse(PRONEWS_RSS)
        print(f"ğŸ” ì´ {len(feed.entries)}ê°œ í”¼ë“œ í•­ëª© ê²€ìƒ‰...")

        unposted = []
        for entry in feed.entries:
            if not FORCE_UPDATE and entry.link in self.posted_articles:
                continue
            try:
                article_date = datetime(*entry.published_parsed[:6])
            except:
                article_date = datetime.now()

            unposted.append({
                'title': entry.title,
                'link': entry.link,
                'date': article_date
            })

        unposted.sort(key=lambda x: x['date'], reverse=True)
        target = unposted[:DAILY_LIMIT]

        print(f"âœ… ë¯¸ê²Œì‹œ: {len(unposted)}ê±´ â†’ ì˜¤ëŠ˜ ì²˜ë¦¬: {len(target)}ê±´ (ìµœëŒ€ {DAILY_LIMIT}ê±´)")
        return target

    def fetch_full_content(self, url: str):
        """ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘ + ë¶ˆí•„ìš” ìš”ì†Œ ì œê±°"""
        try:
            print(f"ğŸ“„ ìŠ¤í¬ë˜í•‘: {url}")
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'lxml')
            content_div = (
                soup.find('div', class_='entry-content') or
                soup.find('div', class_='post-content') or
                soup.find('div', class_='article-content') or
                soup.find('article')
            )
            if not content_div:
                return None

            # ë¶ˆí•„ìš” í…ìŠ¤íŠ¸/ì„¹ì…˜ ì œê±°
            for elem in content_div.find_all(string=re.compile(
                r'ì›ë¬¸ ê²Œì‹œì‹œê°:|ì¶œì²˜:|åŸæ–‡æ²è¼‰æ™‚åˆ»:|ã‚½ãƒ¼ã‚¹:|ãƒãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼|é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰|ã“ã®è¨˜äº‹ã‚’ã‚·ã‚§ã‚¢|FOLLOW US'
            )):
                parent = elem.find_parent()
                if parent:
                    parent.decompose()

            remove_headings = ['ë°± ë„˜ë²„', 'é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'ãƒãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼',
                               'ã“ã®è¨˜äº‹ã‚’ã‚·ã‚§ã‚¢', 'ì´ ê¸°ì‚¬ ê³µìœ ', 'FOLLOW US',
                               'é–¢é€£è¨˜äº‹', 'ê´€ë ¨ ê¸°ì‚¬']
            for h_tag in content_div.find_all(['h2', 'h3', 'h4']):
                if any(kw in h_tag.get_text(strip=True) for kw in remove_headings):
                    next_elem = h_tag.find_next_sibling()
                    h_tag.decompose()
                    while next_elem and next_elem.name not in ['h1', 'h2', 'h3', 'h4']:
                        temp = next_elem.find_next_sibling()
                        next_elem.decompose()
                        next_elem = temp

            for tag in content_div(['script', 'style', 'noscript',
                                    'form', 'nav', 'aside', 'footer', 'header']):
                tag.decompose()

            # ê´‘ê³  iframe ì œê±°, ë™ì˜ìƒ iframe ë³´ì¡´
            video_domains = ['youtube', 'youtu.be', 'vimeo', 'dailymotion', 'player']
            for iframe in list(content_div.find_all('iframe')):
                src = iframe.get('src', '')
                if not any(v in src.lower() for v in video_domains):
                    iframe.decompose()

            # ì›ë¬¸ ì‚¬ì´íŠ¸ ë„¤ë¹„ê²Œì´ì…˜/ì¹´í…Œê³ ë¦¬ ìš”ì†Œ ì œê±°
            nav_keywords = ['ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§', 'ë‰´ìŠ¤ ëª©ë¡', 'ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'å±•ç¤ºãƒ¬ãƒãƒ¼ãƒˆ',
                            'ì „ì‹œ ë¦¬í¬íŠ¸', 'ì „ì‹œíšŒ', 'ã‚³ãƒ©ãƒ ä¸€è¦§', 'ãƒ¬ãƒ“ãƒ¥ãƒ¼ä¸€è¦§']
            for elem in content_div.find_all(['a', 'span', 'div', 'p']):
                text = elem.get_text(strip=True)
                if text and any(kw in text for kw in nav_keywords) and len(text) < 30:
                    elem.decompose()

            social_classes = ['social-share', 'share-buttons', 'sns-share', 'social-links',
                               'share-links', 'addtoany', 'sharedaddy', 'jp-relatedposts',
                               'entry-footer', 'post-tags', 'post-categories', 'post-meta']
            for elem in content_div.find_all(class_=lambda x: x and any(
                sc in ' '.join(x).lower() for sc in social_classes
            )):
                elem.decompose()

            remove_keywords = [
                'facebook.com', 'twitter.com', 'line.me', 'instagram.com',
                'youtube.com', 'pronews.jp', 'kr.pronews.com', '/fellowship/',
                'getpocket.com', 'hatena.ne.jp', '/feed', '/columntitle/',
                '/specialtitle/', '/writer/', 'jp.pronews.com'
            ]
            for a in list(content_div.find_all('a')):
                href = a.get('href', '')
                text = a.get_text(strip=True)
                if any(kw in href.lower() for kw in remove_keywords) or href.startswith('//') or not text:
                    a.decompose()

            for tag_name in ['p', 'div', 'span', 'li']:
                for tag in content_div.find_all(tag_name):
                    if not tag.get_text(strip=True) and not tag.find('img'):
                        tag.decompose()

            return str(content_div)

        except Exception as e:
            print(f"âš ï¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return None

    def generate_slug(self, title_ja: str, article_date: datetime) -> str:
        """ì˜ë¬¸ slug ìƒì„± (ì˜ë¬¸ í‚¤ì›Œë“œ + ë‚ ì§œ)"""
        words = title_ja.split()
        slug_words = []
        for word in words[:6]:
            cleaned = re.sub(r'[^a-zA-Z0-9\-]', '', word.lower())
            if cleaned and len(cleaned) > 2:
                slug_words.append(cleaned)

        date_str = article_date.strftime('%Y%m%d')
        slug = ('-'.join(slug_words[:4]) + f"-{date_str}") if slug_words else f"article-{date_str}-{int(time.time())}"
        return slug[:60]

    def get_main_image_url(self, link: str):
        try:
            res = requests.get(link, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
            soup = BeautifulSoup(res.text, 'lxml')
            og_img = soup.find('meta', property='og:image')
            if og_img and og_img.get('content'):
                return og_img['content']
            content = soup.find('div', class_='entry-content')
            if content:
                img = content.find('img')
                if img and img.get('src'):
                    img_url = img['src']
                    return img_url if img_url.startswith('http') else urljoin(link, img_url)
        except:
            pass
        return None

    def download_image(self, url: str):
        if not url:
            return None
        try:
            print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ: {url}")
            res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=15)
            res.raise_for_status()
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
            original_filename = os.path.basename(urlparse(url).path).split('?')[0]
            ext = os.path.splitext(original_filename)[1]
            if ext not in ['.jpg', '.jpeg', '.png', '.gif', '.webp']:
                ext = '.jpg'
            path = Path(f"/tmp/pronews_{int(time.time())}_{url_hash}{ext}")
            with open(path, 'wb') as f:
                f.write(res.content)
            print(f"   âœ… {path.name}")
            return path
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def upload_media(self, image_path: Path):
        if not image_path or not image_path.exists():
            return None
        try:
            with open(image_path, 'rb') as img:
                res = requests.post(
                    f"{self.wordpress_api}/media",
                    auth=(WORDPRESS_USER, WORDPRESS_APP_PASSWORD),
                    headers={'Content-Disposition': f'attachment; filename={image_path.name}'},
                    files={'file': (image_path.name, img, 'image/jpeg')}
                )
                res.raise_for_status()
                return res.json()
        except Exception as e:
            print(f"âš ï¸ ë¯¸ë””ì–´ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def is_already_posted_on_wp(self, original_url: str) -> bool:
        """WordPressì—ì„œ ì›ë¬¸ URL ê¸°ì¤€ ì¤‘ë³µ ê²Œì‹œ ì—¬ë¶€ í™•ì¸"""
        try:
            search_term = original_url.split('/')[-2] if original_url.endswith('/') else original_url.split('/')[-1]
            res = requests.get(
                f"{self.wordpress_api}/posts",
                auth=(WORDPRESS_USER, WORDPRESS_APP_PASSWORD),
                params={'search': search_term, 'per_page': 5, 'status': 'any'},
                timeout=10
            )
            if res.status_code == 200:
                for post in res.json():
                    if original_url in post.get('content', {}).get('rendered', ''):
                        print(f"âš ï¸ ì¤‘ë³µ ê°ì§€ â†’ ìŠ¤í‚µ: {post['link']}")
                        return True
            return False
        except Exception as e:
            print(f"âš ï¸ ì¤‘ë³µ ì²´í¬ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {e}")
            return False

    def commit_posted_articles(self):
        """posted_articles.json git ì»¤ë°‹ (ìºì‹œ ìœ ì‹¤ ë°©ì§€)"""
        try:
            import subprocess
            subprocess.run(['git', 'config', 'user.email', 'action@github.com'], check=True)
            subprocess.run(['git', 'config', 'user.name', 'GitHub Action'], check=True)
            subprocess.run(['git', 'add', POSTED_ARTICLES_FILE], check=True)
            result = subprocess.run(['git', 'diff', '--cached', '--quiet'], capture_output=True)
            if result.returncode != 0:
                subprocess.run(
                    ['git', 'commit', '-m', f'chore: update posted_articles [{datetime.now().strftime("%Y-%m-%d %H:%M")}]'],
                    check=True
                )
                subprocess.run(['git', 'push'], check=True)
                print("ğŸ“ posted_articles.json â†’ git ì»¤ë°‹ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ git ì»¤ë°‹ ì‹¤íŒ¨ (ìºì‹œë¡œ ëŒ€ì²´): {e}")

    def post_to_wordpress(self, title: str, content: str, slug: str,
                           featured_media_id: int, original_date: datetime,
                           excerpt: str = "", status: str = "publish") -> bool:
        post_data = {
            'title': title,
            'content': content,
            'slug': slug,
            'status': status,
            'featured_media': featured_media_id or 0,
            'date': original_date.strftime('%Y-%m-%dT%H:%M:%S')
        }
        if excerpt:
            post_data['excerpt'] = excerpt
        try:
            res = requests.post(
                f"{self.wordpress_api}/posts",
                auth=(WORDPRESS_USER, WORDPRESS_APP_PASSWORD),
                json=post_data
            )
            res.raise_for_status()
            post_info = res.json()
            label = "ğŸ“ ì„ì‹œì €ì¥" if status == "draft" else "âœ¨ ê²Œì‹œ ì„±ê³µ"
            print(f"{label}: {post_info['link']}")
            return True
        except Exception as e:
            print(f"âŒ ê²Œì‹œ ì‹¤íŒ¨: {e}")
            if hasattr(e, 'response') and e.response is not None:
                print(f"   {e.response.text[:300]}")
            return False

    def process_article(self, article: dict) -> bool:
        print(f"\n{'='*60}")
        print(f"ğŸ“° {article['title'][:70]}")
        print(f"ğŸ“… {article['date'].strftime('%Y-%m-%d %H:%M')}")
        print(f"{'='*60}")

        # 1. ì¤‘ë³µ ì²´í¬ (2ì¤‘ ì•ˆì „ë§)
        if not FORCE_UPDATE and self.is_already_posted_on_wp(article['link']):
            if article['link'] not in self.posted_articles:
                self.posted_articles.append(article['link'])
                self.save_posted_articles()
            return False

        # 2. ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘
        raw_html = self.fetch_full_content(article['link'])
        if not raw_html:
            print("âš ï¸ ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ â†’ ìŠ¤í‚µ")
            return False

        # 3. Gemini ì œëª© ë²ˆì—­ + SEO í¸ì§‘
        print("ğŸ”„ [1ë‹¨ê³„] Gemini ì œëª© ë²ˆì—­+í¸ì§‘ ì¤‘...")
        title_ko = self.gemini.translate_and_edit_title(article['title'])
        if not title_ko:
            print("âŒ ì œëª© ë²ˆì—­ ì‹¤íŒ¨ â†’ ì´ ê¸°ì‚¬ ìŠ¤í‚µ")
            return False

        # 4. Gemini ë³¸ë¬¸ ë²ˆì—­ + SEO í¸ì§‘
        print("âœï¸  [2ë‹¨ê³„] Gemini ë³¸ë¬¸ ë²ˆì—­+í¸ì§‘ ì¤‘...")
        content_ko = self.gemini.translate_and_edit_content(raw_html)
        if not content_ko:
            print("âŒ ë³¸ë¬¸ ë²ˆì—­ ì‹¤íŒ¨ â†’ ì´ ê¸°ì‚¬ ìŠ¤í‚µ")
            return False

        # ìµœì¢… ì•ˆì „ë§: ê²Œì‹œ ì§ì „ ì¼ë³¸ì–´ ì”ì¡´ ê²€ì‚¬
        if self.gemini._has_japanese(content_ko):
            print("âŒ ìµœì¢… ê²€ì‚¬ì—ì„œ ì¼ë³¸ì–´ ë‹¤ìˆ˜ ì”ì¡´ â†’ ì´ ê¸°ì‚¬ ìŠ¤í‚µ")
            return False

        # 5. excerpt ìƒì„±
        excerpt = ""
        if GENERATE_EXCERPT:
            print("ğŸ“‹ [3ë‹¨ê³„] excerpt ìƒì„± ì¤‘...")
            excerpt = self.gemini.generate_excerpt(title_ko, content_ko)
            time.sleep(1)

        # 6. Slug ìƒì„±
        slug = self.generate_slug(article['title'], article['date'])
        print(f"ğŸ”— Slug: {slug}")

        # 7. ì´ë¯¸ì§€ ì²˜ë¦¬
        print("ğŸ” ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘...")
        featured_id = 0
        img_url = self.get_main_image_url(article['link'])
        if img_url:
            local_img = self.download_image(img_url)
            if local_img:
                media_info = self.upload_media(local_img)
                if media_info:
                    featured_id = media_info['id']
                try:
                    local_img.unlink()
                except:
                    pass

        # 8. ìµœì¢… ë³¸ë¬¸ êµ¬ì„± + ì›ë¬¸ ì¶œì²˜
        final_content = content_ko
        final_content += (
            "\n\n<hr style='margin:40px 0 20px 0;border:0;border-top:1px solid #e0e0e0;'>\n"
            f"<p style='font-size:13px;color:#777;'>"
            f"<strong>ì›ë¬¸:</strong> "
            f"<a href='{article['link']}' target='_blank' rel='noopener'>{article['title']}</a>"
            f"</p>"
        )

        # 9. WordPress ê²Œì‹œ
        label = "draft(ì„ì‹œì €ì¥)" if POST_STATUS == "draft" else "publish(ì¦‰ì‹œê³µê°œ)"
        print(f"ğŸ“¤ [4ë‹¨ê³„] WordPress {label} ì¤‘...")
        if self.post_to_wordpress(title_ko, final_content, slug, featured_id,
                                   article['date'], excerpt=excerpt, status=POST_STATUS):
            if not FORCE_UPDATE:
                self.posted_articles.append(article['link'])
                self.save_posted_articles()
            return True
        return False

    def run(self):
        print(f"\n{'='*60}")
        print(f"pronews.jp â†’ prodg.kr ìë™ ë²ˆì—­ v6")
        print(f"ì—”ì§„: Gemini ë‹¨ì¼ ({GEMINI_MODEL})")
        print(f"ê²Œì‹œ: {POST_STATUS.upper()} ({'ì¦‰ì‹œ ê³µê°œ' if POST_STATUS == 'publish' else 'ì„ì‹œì €ì¥ â†’ ìˆ˜ë™ ê²€ìˆ˜'})")
        print(f"ì¼ì¼ í•œë„: ìµœëŒ€ {DAILY_LIMIT}ê±´")
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*60}\n")

        if not WORDPRESS_USER or not WORDPRESS_APP_PASSWORD:
            print("âŒ WP_USER / WP_APP_PASSWORD í™˜ê²½ë³€ìˆ˜ í•„ìš”")
            sys.exit(1)

        # API í‚¤ ìœ íš¨ì„± ì‚¬ì „ í…ŒìŠ¤íŠ¸
        print("ğŸ”‘ Gemini API í‚¤ ê²€ì¦ ì¤‘...")
        test_result = self.gemini._call_api("í•œêµ­ì–´ë¡œ ë²ˆì—­: ãƒ†ã‚¹ãƒˆ", max_tokens=50)
        if not test_result:
            print("âŒ Gemini API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ APIì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("   GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            sys.exit(1)
        print(f"   âœ… API ì‘ë‹µ í™•ì¸: '{test_result}'")

        articles = self.fetch_rss_feed()
        if not articles:
            print("âœ… ì²˜ë¦¬í•  ê¸°ì‚¬ ì—†ìŒ (ëª¨ë‘ ê²Œì‹œ ì™„ë£Œ)")
            return

        success = 0
        for i, article in enumerate(articles, 1):
            print(f"\n[{i}/{len(articles)}]")
            if self.process_article(article):
                success += 1
            time.sleep(3)

        print(f"\n{'='*60}")
        print(f"ğŸ ì™„ë£Œ: {success}/{len(articles)}ê±´ ê²Œì‹œ")
        print(f"{'='*60}\n")

        if success > 0:
            self.commit_posted_articles()


if __name__ == "__main__":
    bot = NewsTranslator()
    bot.run()
