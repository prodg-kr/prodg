#!/usr/bin/env python3
"""
pronews.jp ìë™ ë²ˆì—­ ë° ì›Œë“œí”„ë ˆìŠ¤ ê²Œì‹œ ì‹œìŠ¤í…œ
- ì†ŒìŠ¤: jp.pronews.com WordPress API
- ë²ˆì—­: Google Translate (ì¼ë³¸ì–´ â†’ í•œêµ­ì–´)
- ê²Œì‹œ: prodg.kr WordPress
- ê¸°ëŠ¥: ì „ì²´ ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘, ì´ë¯¸ì§€ ë³¸ë¬¸ ì‚½ì…
"""

import os
import sys
import requests
from datetime import datetime, timedelta, timezone
from pathlib import Path
import json
import time
from urllib.parse import urlparse, urljoin
from googletrans import Translator
import html2text
from bs4 import BeautifulSoup
import re

# ==========================================
# ì„¤ì • (Settings)
# ==========================================
WORDPRESS_URL = "https://prodg.kr"
WORDPRESS_USER = os.environ.get("WP_USER")
WORDPRESS_APP_PASSWORD = os.environ.get("WP_APP_PASSWORD")
PRONEWS_POSTS_API = "https://jp.pronews.com/wp-json/wp/v2/posts"
POSTED_ARTICLES_FILE = "posted_articles.json"
SOURCE_TZ = timezone(timedelta(hours=9))
DAILY_POST_LIMIT = max(1, int(os.environ.get("DAILY_POST_LIMIT", "10")))
SOURCE_SCAN_MAX_PAGES = max(1, int(os.environ.get("SOURCE_SCAN_MAX_PAGES", "60")))
REQUEST_TIMEOUT = int(os.environ.get("REQUEST_TIMEOUT", "20"))

# ì¤‘ë³µ ê²Œì‹œ ë°©ì§€ (Falseë¡œ ì„¤ì •í•˜ë©´ ì´ë¯¸ ì˜¬ë¦° ê¸€ì€ ê±´ë„ˆëœ€)
FORCE_UPDATE = False

class NewsTranslator:
    def __init__(self):
        self.translator = Translator()
        self.wordpress_api = f"{WORDPRESS_URL}/wp-json/wp/v2"
        self.posted_articles = self.load_posted_articles()
        
    def load_posted_articles(self):
        """ì´ë¯¸ ê²Œì‹œëœ ê¸°ì‚¬ ëª©ë¡ ë¡œë“œ"""
        if Path(POSTED_ARTICLES_FILE).exists():
            with open(POSTED_ARTICLES_FILE, 'r') as f:
                try:
                    data = json.load(f)
                    if isinstance(data, list):
                        return set(data)
                    if isinstance(data, dict):
                        return set(data.keys())
                    return set()
                except Exception:
                    return set()
        return set()
        
    def save_posted_articles(self):
        """ê²Œì‹œëœ ê¸°ì‚¬ ëª©ë¡ ì €ì¥"""
        with open(POSTED_ARTICLES_FILE, 'w') as f:
            json.dump(sorted(self.posted_articles), f, indent=2, ensure_ascii=False)

    def normalize_source_url(self, raw_url):
        """ì›ë¬¸ ë„ë©”ì¸ì„ jp.pronews.comìœ¼ë¡œ ì •ê·œí™”"""
        if not raw_url:
            return ""

        normalized = raw_url.strip()
        if not normalized.startswith(("http://", "https://")):
            normalized = f"https://{normalized.lstrip('/')}"

        parsed = urlparse(normalized)
        netloc = parsed.netloc.lower()
        if netloc.startswith("www."):
            netloc = netloc[4:]

        if netloc in {"pronews.jp", "www.pronews.jp", "ko.pronews.com"}:
            netloc = "jp.pronews.com"

        rebuilt = parsed._replace(netloc=netloc).geturl()
        return rebuilt

    def normalize_pronews_domains_in_text(self, text):
        """ë²ˆì—­ ì¤‘ ì˜ëª» ë°”ë€ pronews ë„ë©”ì¸ ë³µêµ¬"""
        if not text:
            return text

        fixed = text
        fixed = re.sub(r"https?://ko\.pronews\.com", "https://jp.pronews.com", fixed)
        fixed = re.sub(r"https?://(?:www\.)?pronews\.jp", "https://jp.pronews.com", fixed)
        return fixed

    def parse_source_datetime(self, date_text=None, date_gmt_text=None):
        """
        ì›ë¬¸ ê²Œì‹œ ì‹œê° íŒŒì‹±.
        - date_gmtê°€ ìˆìœ¼ë©´ UTC ê¸°ì¤€ìœ¼ë¡œ íŒŒì‹±
        - ì—†ìœ¼ë©´ dateë¥¼ JST/KST(+09:00)ë¡œ ì²˜ë¦¬
        """
        try:
            if date_gmt_text:
                dt = datetime.fromisoformat(date_gmt_text.replace("Z", "+00:00"))
                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=timezone.utc)
                return dt.astimezone(SOURCE_TZ)
            if date_text:
                dt = datetime.fromisoformat(date_text.replace("Z", "+00:00"))
                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=SOURCE_TZ)
                return dt.astimezone(SOURCE_TZ)
        except Exception:
            pass
        return datetime.now(SOURCE_TZ)

    def to_wordpress_dates(self, source_dt):
        """WordPress ê²Œì‹œìš© date/date_gmt ìƒì„±"""
        local_dt = source_dt.astimezone(SOURCE_TZ)
        gmt_dt = source_dt.astimezone(timezone.utc)
        return (
            local_dt.strftime("%Y-%m-%dT%H:%M:%S"),
            gmt_dt.strftime("%Y-%m-%dT%H:%M:%S"),
        )
        
    def fetch_source_articles(self):
        """ì›ë¬¸ WordPress APIì—ì„œ ìµœì‹ ìˆœ ê¸°ì‚¬ ìˆ˜ì§‘ (ë¯¸ê²Œì‹œ ìš°ì„ )"""
        print(f"ğŸ“¡ ì›ë¬¸ API í™•ì¸ ì¤‘: {PRONEWS_POSTS_API}")
        headers = {
            "User-Agent": "Mozilla/5.0 (compatible; proDG-bot/1.0)"
        }
        collected = []
        seen_links = set()

        for page in range(1, SOURCE_SCAN_MAX_PAGES + 1):
            try:
                params = {
                    "per_page": 100,
                    "page": page,
                    "orderby": "date",
                    "order": "desc",
                    "_fields": "date,date_gmt,link,title",
                }
                res = requests.get(
                    PRONEWS_POSTS_API,
                    params=params,
                    headers=headers,
                    timeout=REQUEST_TIMEOUT
                )

                # ë§ˆì§€ë§‰ í˜ì´ì§€ ì´í›„ ìš”ì²­ ì‹œ WordPressê°€ 400ì„ ë°˜í™˜í•˜ëŠ” ê²½ìš°ê°€ ìˆìŒ
                if res.status_code == 400:
                    print(f"   â„¹ï¸ í˜ì´ì§€ {page} ì´í›„ ê¸°ì‚¬ ì—†ìŒ")
                    break

                res.raise_for_status()
                posts = res.json()
                if not posts:
                    break

                print(f"   ğŸ” í˜ì´ì§€ {page}: {len(posts)}ê°œ í™•ì¸")
                for post in posts:
                    link = self.normalize_source_url(post.get("link", ""))
                    if not link or link in seen_links:
                        continue
                    seen_links.add(link)

                    if not FORCE_UPDATE and link in self.posted_articles:
                        continue

                    title_html = post.get("title", {}).get("rendered", "")
                    title_text = BeautifulSoup(title_html, "lxml").get_text(" ", strip=True)
                    article_date = self.parse_source_datetime(
                        post.get("date"),
                        post.get("date_gmt")
                    )

                    collected.append({
                        "title": title_text or "ì œëª© ì—†ìŒ",
                        "link": link,
                        "date": article_date,
                    })

                    if len(collected) >= DAILY_POST_LIMIT:
                        break

                if len(collected) >= DAILY_POST_LIMIT:
                    break

            except Exception as e:
                print(f"âš ï¸ ì›ë¬¸ ëª©ë¡ ìˆ˜ì§‘ ì‹¤íŒ¨ (page={page}): {e}")
                break

        # ìµœì‹  ê¸°ì‚¬ë¶€í„° ê²Œì‹œë˜ë„ë¡ ë‚ ì§œ ë‚´ë¦¼ì°¨ìˆœ ë³´ì¥
        collected.sort(key=lambda x: x["date"], reverse=True)
        print(f"âœ… ì²˜ë¦¬í•  ê¸°ì‚¬: {len(collected)}ê°œ (ì¼ì¼ í•œë„: {DAILY_POST_LIMIT})")
        return collected[:DAILY_POST_LIMIT]
        
    def fetch_full_content(self, url):
        """
        BeautifulSoupì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ê¸°ì‚¬ ë³¸ë¬¸ ì „ì²´ ìŠ¤í¬ë˜í•‘
        """
        try:
            print(f"ğŸ“„ ê¸°ì‚¬ ì›ë¬¸ ìŠ¤í¬ë˜í•‘ ì¤‘: {url}")
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'lxml')
            
            # pronews.comì˜ ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸°
            # ì¼ë°˜ì ì¸ ì›Œë“œí”„ë ˆìŠ¤ êµ¬ì¡°: entry-content, post-content, article-content ë“±
            content_div = soup.find('div', class_='entry-content')
            if not content_div:
                content_div = soup.find('div', class_='post-content')
            if not content_div:
                content_div = soup.find('div', class_='article-content')
            if not content_div:
                content_div = soup.find('article')
                
            if not content_div:
                print("âš ï¸ ë³¸ë¬¸ ì˜ì—­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return None

            # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±° (ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼, ê´‘ê³  ë“±)
            for tag in content_div(['script', 'style', 'iframe', 'noscript', 'form', 'nav']):
                tag.decompose()
            
            # ê´‘ê³  í´ë˜ìŠ¤ ì œê±°
            for ad_class in ['ad', 'advertisement', 'banner', 'sidebar']:
                for elem in content_div.find_all(class_=lambda x: x and ad_class in x.lower()):
                    elem.decompose()
                
            # HTML ë¬¸ìì—´ ë°˜í™˜
            return str(content_div)
            
        except Exception as e:
            print(f"âš ï¸ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None

    def translate_text(self, text):
        """ë²ˆì—­ í•¨ìˆ˜ (ê¸´ í…ìŠ¤íŠ¸ ìë™ ë¶„í•  ì²˜ë¦¬)"""
        if not text: 
            return ""
        
        try:
            # HTMLì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            h = html2text.HTML2Text()
            h.ignore_links = False
            h.ignore_images = True  # ì´ë¯¸ì§€ëŠ” ë³„ë„ ì²˜ë¦¬
            h.body_width = 0  # ì¤„ë°”ê¿ˆ ë°©ì§€
            plain_text = h.handle(text)
            
            # ë„ˆë¬´ ê¸¸ë©´ ì²­í¬ë¡œ ë‚˜ëˆ ì„œ ë²ˆì—­ (Google API ì œí•œ ëŒ€ë¹„)
            max_chunk_size = 4000
            if len(plain_text) > max_chunk_size:
                print(f"   ğŸ“ ê¸´ í…ìŠ¤íŠ¸ ê°ì§€ ({len(plain_text)}ì) - ë¶„í•  ë²ˆì—­ ì‹œì‘")
                chunks = [plain_text[i:i+max_chunk_size] for i in range(0, len(plain_text), max_chunk_size)]
                translated_parts = []
                
                for i, chunk in enumerate(chunks, 1):
                    print(f"   ğŸ”„ ì²­í¬ {i}/{len(chunks)} ë²ˆì—­ ì¤‘...")
                    res = self.translator.translate(chunk, src='ja', dest='ko')
                    translated_parts.append(res.text)
                    time.sleep(1.5)  # API ì œí•œ ë°©ì§€
                    
                return "\n\n".join(translated_parts)
            else:
                result = self.translator.translate(plain_text, src='ja', dest='ko')
                time.sleep(0.8)
                return result.text
                
        except Exception as e:
            print(f"âš ï¸ ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return text  # ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ë°˜í™˜

    def download_image(self, url):
        """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
        if not url: 
            return None
        try:
            print(f"ğŸ–¼ï¸  ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ: {url}")
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            res = requests.get(url, headers=headers, timeout=15)
            res.raise_for_status()
            
            # íŒŒì¼ëª… ì²˜ë¦¬
            filename = os.path.basename(urlparse(url).path)
            if not filename or len(filename) > 100:
                filename = f"image_{int(time.time())}.jpg"
            
            # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±°
            if '?' in filename:
                filename = filename.split('?')[0]
                
            path = Path(f"/tmp/{filename}")
            with open(path, 'wb') as f:
                f.write(res.content)
            
            print(f"   âœ… ì €ì¥ ì™„ë£Œ: {path.name}")
            return path
            
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì—ëŸ¬: {e}")
        return None

    def upload_media(self, image_path):
        """ì›Œë“œí”„ë ˆìŠ¤ ë¯¸ë””ì–´ ì—…ë¡œë“œ"""
        if not image_path or not image_path.exists(): 
            return None
        try:
            url = f"{self.wordpress_api}/media"
            headers = {
                'Content-Disposition': f'attachment; filename={image_path.name}'
            }
            with open(image_path, 'rb') as img:
                files = {'file': (image_path.name, img, 'image/jpeg')}
                res = requests.post(
                    url,
                    auth=(WORDPRESS_USER, WORDPRESS_APP_PASSWORD),
                    headers=headers,
                    files=files
                )
                res.raise_for_status()
                media_data = res.json()
                print(f"   âœ… ì—…ë¡œë“œ ì™„ë£Œ: ID {media_data['id']}")
                return media_data  # {id, source_url, ...}
                
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            if hasattr(e, 'response'):
                print(f"   ìƒì„¸: {e.response.text[:200]}")
        return None

    def get_main_image_url(self, link):
        """Open Graph ë“±ì„ í†µí•´ ëŒ€í‘œ ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
        try:
            headers = {'User-Agent': 'Mozilla/5.0'}
            res = requests.get(link, headers=headers, timeout=10)
            soup = BeautifulSoup(res.text, 'lxml')
            
            # 1. Open Graph ì´ë¯¸ì§€
            og_img = soup.find('meta', property='og:image')
            if og_img and og_img.get('content'):
                img_url = og_img['content']
                print(f"   ğŸ“¸ OG ì´ë¯¸ì§€ ë°œê²¬")
                return img_url
            
            # 2. Twitter Card ì´ë¯¸ì§€
            tw_img = soup.find('meta', attrs={'name': 'twitter:image'})
            if tw_img and tw_img.get('content'):
                img_url = tw_img['content']
                print(f"   ğŸ“¸ Twitter Card ì´ë¯¸ì§€ ë°œê²¬")
                return img_url
            
            # 3. ë³¸ë¬¸ ì²« ì´ë¯¸ì§€
            content = soup.find('div', class_='entry-content')
            if content:
                img = content.find('img')
                if img and img.get('src'):
                    img_url = img['src']
                    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ
                    if not img_url.startswith('http'):
                        img_url = urljoin(link, img_url)
                    print(f"   ğŸ“¸ ë³¸ë¬¸ ì´ë¯¸ì§€ ë°œê²¬")
                    return img_url
            
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return None

    def post_to_wordpress(self, title, content, featured_media_id, article_date):
        """ì›Œë“œí”„ë ˆìŠ¤ í¬ìŠ¤íŠ¸ ìƒì„±"""
        post_date, post_date_gmt = self.to_wordpress_dates(article_date)
        post_data = {
            'title': title,
            'content': content,
            'status': 'publish',
            'featured_media': featured_media_id if featured_media_id else 0,
            'date': post_date,
            'date_gmt': post_date_gmt
        }
        
        try:
            res = requests.post(
                f"{self.wordpress_api}/posts",
                auth=(WORDPRESS_USER, WORDPRESS_APP_PASSWORD),
                json=post_data
            )
            res.raise_for_status()
            post_info = res.json()
            print(f"âœ¨ ê²Œì‹œ ì„±ê³µ! ë§í¬: {post_info['link']}")
            return True
            
        except Exception as e:
            print(f"âŒ ê²Œì‹œ ì‹¤íŒ¨: {e}")
            if hasattr(e, 'response'):
                print(f"   ìƒì„¸: {e.response.text[:300]}")
            return False

    def process_article(self, article):
        """ê¸°ì‚¬ í•˜ë‚˜ ì²˜ë¦¬: ìŠ¤í¬ë˜í•‘ â†’ ë²ˆì—­ â†’ ì´ë¯¸ì§€ â†’ ê²Œì‹œ"""
        print(f"\n{'='*70}")
        source_date = article["date"].astimezone(SOURCE_TZ)
        print(f"ğŸ“° ì²˜ë¦¬ ì‹œì‘: {article['title']}")
        print(f"ğŸ•’ ì›ë¬¸ ê²Œì‹œì‹œê°: {source_date.strftime('%Y-%m-%d %H:%M:%S %z')}")
        print(f"{'='*70}")
        
        # 1. ë³¸ë¬¸ ì „ì²´ ê°€ì ¸ì˜¤ê¸°
        raw_html = self.fetch_full_content(article['link'])
        if not raw_html:
            print("   âš ï¸  ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ì§€ ëª»í•´ ê±´ë„ˆëœë‹ˆë‹¤.")
            return False
            
        # 2. ë²ˆì—­ (ì œëª© ë° ë³¸ë¬¸)
        print(f"ğŸ”„ ì œëª© ë²ˆì—­ ì¤‘...")
        title_ko = self.translate_text(article['title'])
        print(f"   âœ… \"{title_ko}\"")
        
        print(f"ğŸ”„ ë³¸ë¬¸ ë²ˆì—­ ì¤‘...")
        content_ko = self.translate_text(raw_html)
        print(f"   âœ… ë³¸ë¬¸ ë²ˆì—­ ì™„ë£Œ ({len(content_ko)}ì)")
        
        # 3. ì´ë¯¸ì§€ ì²˜ë¦¬
        print(f"ğŸ” ì´ë¯¸ì§€ ê²€ìƒ‰ ì¤‘...")
        img_url = self.get_main_image_url(article['link'])
        featured_id = 0
        uploaded_img_url = ""
        
        if img_url:
            local_img = self.download_image(img_url)
            if local_img:
                media_info = self.upload_media(local_img)
                if media_info:
                    featured_id = media_info['id']
                    uploaded_img_url = media_info['source_url']
                    
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                try: 
                    local_img.unlink()
                except: 
                    pass
        else:
            print("   â„¹ï¸  ì´ë¯¸ì§€ ì—†ìŒ")

        # 4. ë³¸ë¬¸ êµ¬ì„± (ì´ë¯¸ì§€ ì‚½ì… + ì›ë³¸ ë§í¬)
        final_content = ""
        normalized_source_link = self.normalize_source_url(article["link"])
        
        # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ë³¸ë¬¸ ìµœìƒë‹¨ì— ì‚½ì…
        if uploaded_img_url:
            final_content += f'<figure style="margin: 0 0 30px 0;">'
            final_content += f'<img src="{uploaded_img_url}" alt="{title_ko}" style="width:100%; height:auto; display:block;" />'
            final_content += f'</figure>\n\n'

        # ë³¸ë¬¸ ë©”íƒ€ + ë³¸ë¬¸ ë‚´ìš©
        final_content += "<div class='pronews-kr-article' style='font-family: \"Noto Sans KR\", sans-serif; line-height:1.85; font-size:17px;'>"
        final_content += "<div style='border-top:2px solid #111; border-bottom:1px solid #ddd; padding:10px 0; margin:0 0 24px 0;'>"
        final_content += f"<p style='margin:0; color:#555; font-size:13px;'>ì›ë¬¸ ê²Œì‹œì‹œê°: {source_date.strftime('%Y-%m-%d %H:%M')} (JST)</p>"
        final_content += f"<p style='margin:6px 0 0 0; color:#111; font-size:13px;'>ì¶œì²˜: <a href='{normalized_source_link}' target='_blank' rel='noopener'>jp.pronews.com</a></p>"
        final_content += "</div>"
        final_content += self.normalize_pronews_domains_in_text(content_ko).replace("\n", "<br>\n")
        final_content += "</div>"

        # ì›ë¬¸ ë§í¬ ì¶”ê°€
        final_content += f"\n\n<hr style='margin: 40px 0 20px 0;'>\n"
        final_content += f"<p style='font-size: 14px; color: #666;'>"
        final_content += f"â„¹ï¸ <strong>ì›ë¬¸ ê¸°ì‚¬ ë³´ê¸°:</strong> "
        final_content += f"<a href='{normalized_source_link}' target='_blank' rel='noopener'>{article['title']}</a>"
        final_content += f"</p>"
        
        # 5. ì›Œë“œí”„ë ˆìŠ¤ì— ê²Œì‹œ
        print(f"ğŸ“¤ ì›Œë“œí”„ë ˆìŠ¤ ê²Œì‹œ ì¤‘...")
        if self.post_to_wordpress(title_ko, final_content, featured_id, article["date"]):
            if not FORCE_UPDATE:
                self.posted_articles.add(normalized_source_link)
                self.save_posted_articles()
            return True
        return False

    def run(self):
        """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
        print(f"\n{'ğŸš€'*35}")
        print(f"  pronews.jp ìë™ ë²ˆì—­ ì‹œìŠ¤í…œ ì‹œì‘")
        print(f"  ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'ğŸš€'*35}\n")
        
        # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
        if not WORDPRESS_USER or not WORDPRESS_APP_PASSWORD:
            print("âŒ í™˜ê²½ ë³€ìˆ˜ ì„¤ì • í•„ìš”!")
            print("   WP_USERì™€ WP_APP_PASSWORDë¥¼ GitHub Secretsì— ì¶”ê°€í•˜ì„¸ìš”.")
            sys.exit(1)

        # ì›ë¬¸ WordPress APIì—ì„œ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°
        articles = self.fetch_source_articles()
        
        if not articles:
            print("â„¹ï¸  ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ê° ê¸°ì‚¬ ì²˜ë¦¬
        success_count = 0
        for article in articles:
            if self.process_article(article):
                success_count += 1
            time.sleep(3)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
            
        print(f"\n{'='*70}")
        print(f"ğŸ ì‘ì—… ì™„ë£Œ: {success_count}/{len(articles)}ê°œ ê¸°ì‚¬ ê²Œì‹œë¨")
        print(f"{'='*70}\n")

if __name__ == "__main__":
    bot = NewsTranslator()
    bot.run()
