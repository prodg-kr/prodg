#!/usr/bin/env python3
"""
pronews.jp ìë™ ë²ˆì—­ ì‹œìŠ¤í…œ v6
íŒŒì´í”„ë¼ì¸: ì¼ë³¸ì–´ ì›ë¬¸ â†’ Gemini ë²ˆì—­+SEOí¸ì§‘ í†µí•© â†’ WordPress ê²Œì‹œ

v5 â†’ v6 ë³€ê²½ì‚¬í•­:
- Groq ì œê±° â†’ Gemini ë‹¨ì¼ ì—”ì§„ìœ¼ë¡œ í†µí•©
  (Groq llama-3.3-70bì˜ ì¼ë³¸ì–´â†’í•œêµ­ì–´ í’ˆì§ˆ ë¬¸ì œ í•´ê²°)
- ë²ˆì—­+SEOí¸ì§‘ì„ ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ë¡œ ì²˜ë¦¬ (ë¬¸ë§¥ ì¼ê´€ì„± í–¥ìƒ)
- ì œëª© ì˜ë¦¼ ë¬¸ì œ ìˆ˜ì •: ì œí’ˆëª… í¬í•¨ ì‹œ 50ìê¹Œì§€ í—ˆìš©
- ì¼ë³¸ì–´ ì”ì¡´ ê°ì§€ í›„ ì¬ë²ˆì—­ ì•ˆì „ë§ ì¶”ê°€
- POST_STATUS: publish / draft ì„ íƒ ê°€ëŠ¥
- excerpt ìë™ ìƒì„±
- ì¤‘ë³µ ë°©ì§€: posted_articles.json + WordPress API 2ì¤‘ ì²´í¬
- ê²Œì‹œ í›„ posted_articles.json git ìë™ ì»¤ë°‹
"""

import os
import sys
import requests
import feedparser
from datetime import datetime
from pathlib import Path
import json
import time
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
import hashlib
import re

# ==========================================
# ì„¤ì •
# ==========================================
WORDPRESS_URL          = "https://prodg.kr"
WORDPRESS_USER         = os.environ.get("WP_USER")
WORDPRESS_APP_PASSWORD = os.environ.get("WP_APP_PASSWORD")
GEMINI_API_KEY         = os.environ.get("GEMINI_API_KEY")
PRONEWS_RSS            = "https://jp.pronews.com/feed"
POSTED_ARTICLES_FILE   = "posted_articles.json"
FORCE_UPDATE           = os.environ.get("FORCE_UPDATE", "false").lower() == "true"
DAILY_LIMIT            = 10  # í•˜ë£¨ ìµœëŒ€ ê²Œì‹œ ê±´ìˆ˜

# ê²Œì‹œ ìƒíƒœ: publish(ì¦‰ì‹œê³µê°œ) / draft(ì„ì‹œì €ì¥ í›„ ìˆ˜ë™ ê²€ìˆ˜)
POST_STATUS      = os.environ.get("POST_STATUS", "publish")
GENERATE_EXCERPT = True  # WordPress SEOìš© ìš”ì•½ë¬¸ ìë™ ìƒì„±

# ëª¨ë¸ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ë¡œ êµì²´ ê°€ëŠ¥)
GEMINI_MODEL = os.environ.get("GEMINI_MODEL", "gemini-2.5-flash")


# ==========================================
# Gemini í†µí•© ì—”ì§„ (ë²ˆì—­ + SEO í¸ì§‘)
# ==========================================
class GeminiEngine:
    """
    Gemini ë‹¨ì¼ ì—”ì§„ìœ¼ë¡œ ë²ˆì—­+SEOí¸ì§‘ í†µí•© ì²˜ë¦¬
    - ì¼ë³¸ì–´ â†’ í•œêµ­ì–´ ë²ˆì—­ (Groq ëŒ€ë¹„ í’ˆì§ˆ ëŒ€í­ í–¥ìƒ)
    - SEO ìµœì í™” ì œëª© ì¬ì‘ì„±
    - í•©ì‡¼ì²´(~í•©ë‹ˆë‹¤) ë¬¸ì²´ í†µì¼
    - ì „ë¬¸ìš©ì–´ ì •í™•ì„± ë³´ì •
    - excerpt ìƒì„±
    """

    def __init__(self):
        self.api_key = GEMINI_API_KEY
        if not self.api_key:
            print("âŒ GEMINI_API_KEY ë¯¸ì„¤ì •")
            sys.exit(1)

    def _call_api(self, prompt: str, max_tokens: int = 4096) -> str:
        url = (
            f"https://generativelanguage.googleapis.com/v1beta/models/"
            f"{GEMINI_MODEL}:generateContent?key={self.api_key}"
        )
        payload = {
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {
                "maxOutputTokens": max_tokens,
                "temperature": 0.4
            }
        }
        try:
            res = requests.post(url, json=payload, timeout=90)
            res.raise_for_status()
            candidates = res.json().get("candidates", [])
            if candidates:
                return candidates[0]["content"]["parts"][0]["text"].strip()
            return ""
        except Exception as e:
            print(f"âš ï¸ Gemini API ì˜¤ë¥˜: {e}")
            return ""

    def translate_and_edit_title(self, title_ja: str) -> str:
        """
        ì œëª© ë²ˆì—­ + SEO í¸ì§‘ í†µí•©
        - ì œí’ˆëª…/ëª¨ë¸ëª… ì ˆëŒ€ ì˜ë¦¬ì§€ ì•Šë„ë¡ ë³´í˜¸
        - ì œí’ˆëª… í¬í•¨ ì‹œ 50ìê¹Œì§€ í—ˆìš©, ì¼ë°˜ ì œëª©ì€ 35ì ë‚´ì™¸
        """
        prompt = f"""ë‹¹ì‹ ì€ ì˜ìƒ/ì¹´ë©”ë¼ ì „ë¬¸ ë¯¸ë””ì–´ì˜ SEO ì—ë””í„°ì…ë‹ˆë‹¤.

ì¼ë³¸ì–´ ì œëª©: {title_ja}

ìœ„ ì œëª©ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ê³  êµ¬ê¸€ SEOì— ìµœì í™”í•˜ì„¸ìš”.

ê·œì¹™:
1. Sony, Canon, Nikon, DJI, Blackmagic, NIKKOR, LUMIX, FUJIFILM ë“± ë¸Œëœë“œëª…/ì œí’ˆëª…/ëª¨ë¸ëª…ì€ ì›ë¬¸ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³  ì ˆëŒ€ ìƒëµí•˜ì§€ ë§ˆì„¸ìš”
2. ëª¨ë¸ ë²ˆí˜¸(ì˜ˆ: NIKKOR Z 70-200mm f/2.8 VR S II)ê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì „ì²´ í¬í•¨
3. ê²€ìƒ‰ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì•ìª½ì— ë°°ì¹˜
4. ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ (ì§ì—­ì²´, ì–´ìƒ‰í•œ ì¡°ì‚¬ ê¸ˆì§€)
5. ì œí’ˆëª… ì—†ëŠ” ê²½ìš° 35ì ë‚´ì™¸, ì œí’ˆëª… í¬í•¨ ì‹œ 50ìê¹Œì§€ í—ˆìš©
6. ì œëª©ë§Œ ì¶œë ¥ (ì„¤ëª…, ë”°ì˜´í‘œ, ë²ˆí˜¸ ì—†ìŒ)"""

        result = self._call_api(prompt, max_tokens=150)
        if result:
            result = re.sub(r'^[\d\.\)\-\s"\'ã€Œã€ã€ã€‘]+', '', result).strip().strip('"\'ã€Œã€ã€ã€‘')
            print(f"   ğŸ“Œ ë²ˆì—­ ì œëª©: {result}")
            return result
        return title_ja

    def translate_and_edit_content(self, html_content: str) -> str:
        """
        ë³¸ë¬¸ ë²ˆì—­ + SEO í¸ì§‘ í†µí•© (Gemini ë‹¨ì¼ ì²˜ë¦¬)
        - ì¼ë³¸ì–´ â†’ í•œêµ­ì–´ ë²ˆì—­ (ë¬¸ë§¥ ì¼ê´€ì„± ìœ ì§€)
        - ì§ì—­ì²´ â†’ ìì—°ìŠ¤ëŸ¬ìš´ í•©ì‡¼ì²´
        - HTML íƒœê·¸ êµ¬ì¡° ìœ ì§€
        - ì¼ë³¸ì–´ ì”ì¡´ ì‹œ ì¬ë²ˆì—­
        """
        if not html_content:
            return ""

        soup = BeautifulSoup(html_content, 'lxml')

        # ì´ë¯¸ì§€ íƒœê·¸ ë³´ì¡´
        images = {}
        for i, img in enumerate(soup.find_all('img')):
            placeholder = f"___IMG_{i}___"
            images[placeholder] = str(img)
            img.replace_with(placeholder)

        # í—¤ë” íƒœê·¸ ë³´ì¡´
        headers_map = {}
        for i, tag in enumerate(soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])):
            placeholder = f"___H{i}_{tag.name}___"
            headers_map[placeholder] = {'tag': tag.name, 'text': tag.get_text(strip=True)}
            tag.replace_with(placeholder)

        # ë‹¨ë½ ë‹¨ìœ„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        paragraphs = []
        for elem in soup.find_all(['p', 'li', 'blockquote']):
            text = elem.get_text(separator=' ', strip=True)
            if text and len(text) > 5:
                paragraphs.append(text)

        if not paragraphs:
            full_text = soup.get_text(separator='\n', strip=True)
            paragraphs = [line for line in full_text.split('\n') if line.strip()]

        if not paragraphs:
            return html_content

        # ì²­í¬ ë‹¨ìœ„ ë²ˆì—­+í¸ì§‘ (Gemini í† í° í•œë„ ê³ ë ¤, ì²­í¬ë‹¹ 3000ì)
        translated_paragraphs = []
        chunk, chunk_size = [], 0

        for para in paragraphs:
            if chunk_size + len(para) > 3000 and chunk:
                result = self._translate_chunk('\n\n'.join(chunk))
                translated_paragraphs.extend(result.split('\n\n'))
                chunk, chunk_size = [], 0
                time.sleep(1)
            chunk.append(para)
            chunk_size += len(para)

        if chunk:
            result = self._translate_chunk('\n\n'.join(chunk))
            translated_paragraphs.extend(result.split('\n\n'))

        # HTML ì¬ì¡°ë¦½
        translated_html = ""
        for para in translated_paragraphs:
            para = para.strip()
            if not para:
                continue
            if para.startswith('___'):
                translated_html += para + "\n"
            else:
                translated_html += f"<p>{para}</p>\n"

        # í—¤ë” íƒœê·¸ ë³µì› (í—¤ë” í…ìŠ¤íŠ¸ë„ ë²ˆì—­)
        for placeholder, info in headers_map.items():
            if placeholder in translated_html:
                header_ko = self._translate_single(info['text']) if info['text'] else info['text']
                translated_html = translated_html.replace(
                    placeholder,
                    f"<{info['tag']}>{header_ko}</{info['tag']}>"
                )

        # ì´ë¯¸ì§€ íƒœê·¸ ë³µì›
        for placeholder, img_tag in images.items():
            translated_html = translated_html.replace(placeholder, img_tag)

        # ì¼ë³¸ì–´ ì”ì¡´ ê²€ì‚¬ â†’ ì”ì¡´ ì‹œ ì¬ë²ˆì—­
        if self._has_japanese(translated_html):
            print("   âš ï¸ ì¼ë³¸ì–´ ì”ì¡´ ê°ì§€ â†’ ì¬ë²ˆì—­ ì‹œë„...")
            translated_html = self._cleanup_japanese(translated_html)

        return translated_html

    def _translate_chunk(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì²­í¬ ë²ˆì—­ + SEO í¸ì§‘ í†µí•© í”„ë¡¬í”„íŠ¸"""
        if not text.strip():
            return text

        prompt = f"""ë‹¹ì‹ ì€ ì˜ìƒ/ì¹´ë©”ë¼ ì „ë¬¸ ë¯¸ë””ì–´ì˜ í•œêµ­ì–´ ì—ë””í„°ì…ë‹ˆë‹¤.

ì•„ë˜ ì¼ë³¸ì–´ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ í¸ì§‘í•˜ì„¸ìš”.

ë²ˆì—­+í¸ì§‘ ê·œì¹™:
1. ì¼ë³¸ì–´ë¥¼ ì™„ì „íˆ í•œêµ­ì–´ë¡œ ë²ˆì—­ (íˆë¼ê°€ë‚˜Â·ê°€íƒ€ì¹´ë‚˜Â·í•œì ë‹¨ì–´ ì ˆëŒ€ ë‚¨ê¸°ì§€ ë§ ê²ƒ)
2. ë¬¸ì²´ëŠ” ë°˜ë“œì‹œ '~í•©ë‹ˆë‹¤', '~í–ˆìŠµë‹ˆë‹¤', '~ì…ë‹ˆë‹¤' í•©ì‡¼ì²´ë¡œ í†µì¼
   ('~í•œë‹¤', '~í–ˆë‹¤', '~ì´ë‹¤' í‰ì„œì²´ ì‚¬ìš© ê¸ˆì§€)
3. ì§ì—­ì²´, ì–´ìƒ‰í•œ ì¡°ì‚¬, ì¼ë³¸ì‹ í‘œí˜„ì„ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ìˆ˜ì •
4. ì˜ìƒ/ì¹´ë©”ë¼ ì „ë¬¸ìš©ì–´ ì •í™•íˆ í‘œê¸°:
   - ë¸Œëœë“œëª…: Sony, Canon, Nikon, DJI, Blackmagic, DaVinci Resolve ë“± ì›ë¬¸ ìœ ì§€
   - í•´ìƒë„: 4K, 8K, Full HD / í”„ë ˆì„ë ˆì´íŠ¸: fps, 24p, 60p
   - ê¸°íƒ€: ì½”ë±, ë¹„íŠ¸ë ˆì´íŠ¸, ì¡°ë¦¬ê°œ, ì…”í„°ìŠ¤í”¼ë“œ, ë³´ì¼€, ì†ë–¨ë¦¼ë³´ì • ë“±
5. ___IMG_0___, ___H0_h2___ ê°™ì€ í”Œë ˆì´ìŠ¤í™€ë”ëŠ” ì ˆëŒ€ ë³€ê²½í•˜ì§€ ë§ ê²ƒ
6. ë‹¨ë½ êµ¬ì¡° ìœ ì§€ (ë¹ˆ ì¤„ë¡œ ë‹¨ë½ êµ¬ë¶„)
7. ë²ˆì—­ëœ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥ (ì„¤ëª… ì—†ìŒ)

ì¼ë³¸ì–´ í…ìŠ¤íŠ¸:
{text}"""

        result = self._call_api(prompt, max_tokens=4096)
        return result if result else text

    def _translate_single(self, text: str) -> str:
        """ë‹¨ì¼ ì§§ì€ í…ìŠ¤íŠ¸ ë²ˆì—­ (í—¤ë”ìš©)"""
        if not text.strip():
            return text
        prompt = f"ë‹¤ìŒ ì¼ë³¸ì–´ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ í•©ì‡¼ì²´ë¡œ ë²ˆì—­í•˜ì„¸ìš”. ë²ˆì—­ë¬¸ë§Œ ì¶œë ¥:\n{text}"
        result = self._call_api(prompt, max_tokens=200)
        return result if result else text

    def _has_japanese(self, text: str) -> bool:
        """ì¼ë³¸ì–´(íˆë¼ê°€ë‚˜Â·ê°€íƒ€ì¹´ë‚˜) ì”ì¡´ ì—¬ë¶€ ê²€ì‚¬"""
        japanese_pattern = re.compile(r'[\u3040-\u309f\u30a0-\u30ff]')
        plain_text = BeautifulSoup(text, 'lxml').get_text()
        matches = japanese_pattern.findall(plain_text)
        return len(matches) > 5  # 5ì ì´ìƒ ì¼ë³¸ì–´ ì”ì¡´ ì‹œ ì¬ë²ˆì—­

    def _cleanup_japanese(self, html: str) -> str:
        """ì¼ë³¸ì–´ ì”ì¡´ ë¶€ë¶„ë§Œ ì¬ë²ˆì—­"""
        soup = BeautifulSoup(html, 'lxml')
        for elem in soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'li']):
            text = elem.get_text()
            if re.search(r'[\u3040-\u309f\u30a0-\u30ff]', text):
                translated = self._translate_single(text)
                if translated:
                    elem.string = translated
        return str(soup.find('body') or soup)

    def generate_excerpt(self, title_ko: str, content_ko: str) -> str:
        """
        WordPress SEOìš© ìš”ì•½ë¬¸(excerpt) ìƒì„±
        - 80ì ë‚´ì™¸, ê²€ìƒ‰ê²°ê³¼ ìŠ¤ë‹ˆí«ì— ìµœì í™”
        """
        soup = BeautifulSoup(content_ko, 'lxml')
        plain_text = soup.get_text(separator=' ', strip=True)[:500]

        prompt = f"""ë‹¹ì‹ ì€ SEO ì „ë¬¸ ì—ë””í„°ì…ë‹ˆë‹¤.

ê¸°ì‚¬ ì œëª©: {title_ko}
ë³¸ë¬¸ ì¼ë¶€: {plain_text}

êµ¬ê¸€ ê²€ìƒ‰ê²°ê³¼ì— ë…¸ì¶œë  ìš”ì•½ë¬¸(ë©”íƒ€ ë””ìŠ¤í¬ë¦½ì…˜)ì„ ì‘ì„±í•˜ì„¸ìš”.

ê·œì¹™:
1. 80ì ë‚´ì™¸ (ìµœëŒ€ 100ì)
2. í•µì‹¬ í‚¤ì›Œë“œ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨
3. ë…ìê°€ í´ë¦­í•˜ê³  ì‹¶ì–´ì§€ëŠ” ë¬¸ì¥
4. ~í•©ë‹ˆë‹¤ í•©ì‡¼ì²´ë¡œ ì‘ì„±
5. ìš”ì•½ë¬¸ë§Œ ì¶œë ¥ (ì„¤ëª… ì—†ìŒ)"""

        result = self._call_api(prompt, max_tokens=150)
        if result:
            result = result.strip().strip('"\'')
            print(f"   ğŸ“‹ ìš”ì•½ë¬¸: {result[:60]}...")
            return result
        return ""


# ==========================================
# ë©”ì¸ ë²ˆì—­ ì‹œìŠ¤í…œ
# ==========================================
class NewsTranslator:
    def __init__(self):
        self.gemini = GeminiEngine()
        self.wordpress_api = f"{WORDPRESS_URL}/wp-json/wp/v2"
        self.posted_articles = self.load_posted_articles()

    def load_posted_articles(self) -> list:
        if Path(POSTED_ARTICLES_FILE).exists():
            with open(POSTED_ARTICLES_FILE, 'r') as f:
                try:
                    return json.load(f)
                except:
                    return []
        return []

    def save_posted_articles(self):
        with open(POSTED_ARTICLES_FILE, 'w') as f:
            json.dump(self.posted_articles, f, indent=2)

    def fetch_rss_feed(self) -> list:
        """
        RSS í”¼ë“œì—ì„œ ë¯¸ê²Œì‹œ ê¸°ì‚¬ ì¡°íšŒ
        - ìµœì‹ ìˆœ ì •ë ¬
        - ìµœì‹  ê¸°ì‚¬ ë¶€ì¡± ì‹œ ê³¼ê±° ë¯¸ê²Œì‹œ ê¸°ì‚¬ë¡œ ì±„ì›Œ ìµœëŒ€ 10ê±´ ë°˜í™˜
        """
        print(f"ğŸ“¡ RSS í”¼ë“œ í™•ì¸ ì¤‘: {PRONEWS_RSS}")
        feed = feedparser.parse(PRONEWS_RSS)
        print(f"ğŸ” ì´ {len(feed.entries)}ê°œ í”¼ë“œ í•­ëª© ê²€ìƒ‰...")

        unposted = []
        for entry in feed.entries:
            if not FORCE_UPDATE and entry.link in self.posted_articles:
                continue
            try:
                article_date = datetime(*entry.published_parsed[:6])
            except:
                article_date = datetime.now()

            unposted.append({
                'title': entry.title,
                'link': entry.link,
                'date': article_date
            })

        unposted.sort(key=lambda x: x['date'], reverse=True)
        target = unposted[:DAILY_LIMIT]

        print(f"âœ… ë¯¸ê²Œì‹œ: {len(unposted)}ê±´ â†’ ì˜¤ëŠ˜ ì²˜ë¦¬: {len(target)}ê±´ (ìµœëŒ€ {DAILY_LIMIT}ê±´)")
        return target

    def fetch_full_content(self, url: str):
        """ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘ + ë¶ˆí•„ìš” ìš”ì†Œ ì œê±°"""
        try:
            print(f"ğŸ“„ ìŠ¤í¬ë˜í•‘: {url}")
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'lxml')
            content_div = (
                soup.find('div', class_='entry-content') or
                soup.find('div', class_='post-content') or
                soup.find('div', class_='article-content') or
                soup.find('article')
            )
            if not content_div:
                return None

            # ë¶ˆí•„ìš” í…ìŠ¤íŠ¸/ì„¹ì…˜ ì œê±°
            for elem in content_div.find_all(string=re.compile(
                r'ì›ë¬¸ ê²Œì‹œì‹œê°:|ì¶œì²˜:|åŸæ–‡æ²è¼‰æ™‚åˆ»:|ã‚½ãƒ¼ã‚¹:|ãƒãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼|é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰|ã“ã®è¨˜äº‹ã‚’ã‚·ã‚§ã‚¢|FOLLOW US'
            )):
                parent = elem.find_parent()
                if parent:
                    parent.decompose()

            remove_headings = ['ë°± ë„˜ë²„', 'é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'ãƒãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼',
                               'ã“ã®è¨˜äº‹ã‚’ã‚·ã‚§ã‚¢', 'ì´ ê¸°ì‚¬ ê³µìœ ', 'FOLLOW US',
                               'é–¢é€£è¨˜äº‹', 'ê´€ë ¨ ê¸°ì‚¬']
            for h_tag in content_div.find_all(['h2', 'h3', 'h4']):
                if any(kw in h_tag.get_text(strip=True) for kw in remove_headings):
                    next_elem = h_tag.find_next_sibling()
                    h_tag.decompose()
                    while next_elem and next_elem.name not in ['h1', 'h2', 'h3', 'h4']:
                        temp = next_elem.find_next_sibling()
                        next_elem.decompose()
                        next_elem = temp

            for tag in content_div(['script', 'style', 'iframe', 'noscript',
                                    'form', 'nav', 'aside', 'footer', 'header']):
                tag.decompose()

            social_classes = ['social-share', 'share-buttons', 'sns-share', 'social-links',
                               'share-links', 'addtoany', 'sharedaddy', 'jp-relatedposts',
                               'entry-footer', 'post-tags', 'post-categories', 'post-meta']
            for elem in content_div.find_all(class_=lambda x: x and any(
                sc in ' '.join(x).lower() for sc in social_classes
            )):
                elem.decompose()

            remove_keywords = [
                'facebook.com', 'twitter.com', 'line.me', 'instagram.com',
                'youtube.com', 'pronews.jp', 'kr.pronews.com', '/fellowship/',
                'getpocket.com', 'hatena.ne.jp', '/feed', '/columntitle/',
                '/specialtitle/', '/writer/', 'jp.pronews.com'
            ]
            for a in list(content_div.find_all('a')):
                href = a.get('href', '')
                text = a.get_text(strip=True)
                if any(kw in href.lower() for kw in remove_keywords) or href.startswith('//') or not text:
                    a.decompose()

            for tag_name in ['p', 'div', 'span', 'li']:
                for tag in content_div.find_all(tag_name):
                    if not tag.get_text(strip=True) and not tag.find('img'):
                        tag.decompose()

            return str(content_div)

        except Exception as e:
            print(f"âš ï¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return None

    def generate_slug(self, title_ja: str, article_date: datetime) -> str:
        """ì˜ë¬¸ slug ìƒì„± (ì˜ë¬¸ í‚¤ì›Œë“œ + ë‚ ì§œ)"""
        words = title_ja.split()
        slug_words = []
        for word in words[:6]:
            cleaned = re.sub(r'[^a-zA-Z0-9\-]', '', word.lower())
            if cleaned and len(cleaned) > 2:
                slug_words.append(cleaned)

        date_str = article_date.strftime('%Y%m%d')
        slug = ('-'.join(slug_words[:4]) + f"-{date_str}") if slug_words else f"article-{date_str}-{int(time.time())}"
        return slug[:60]

    def get_main_image_url(self, link: str):
        try:
            res = requests.get(link, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
            soup = BeautifulSoup(res.text, 'lxml')
            og_img = soup.find('meta', property='og:image')
            if og_img and og_img.get('content'):
                return og_img['content']
            content = soup.find('div', class_='entry-content')
            if content:
                img = content.find('img')
                if img and img.get('src'):
                    img_url = img['src']
                    return img_url if img_url.startswith('http') else urljoin(link, img_url)
        except:
            pass
        return None

    def download_image(self, url: str):
        if not url:
            return None
        try:
            print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ: {url}")
            res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=15)
            res.raise_for_status()
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
            original_filename = os.path.basename(urlparse(url).path).split('?')[0]
            ext = os.path.splitext(original_filename)[1]
            if ext not in ['.jpg', '.jpeg', '.png', '.gif', '.webp']:
                ext = '.jpg'
            path = Path(f"/tmp/pronews_{int(time.time())}_{url_hash}{ext}")
            with open(path, 'wb') as f:
                f.write(res.content)
            print(f"   âœ… {path.name}")
            return path
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def upload_media(self, image_path: Path):
        if not image_path or not image_path.exists():
            return None
        try:
            with open(image_path, 'rb') as img:
                res = requests.post(
                    f"{self.wordpress_api}/media",
                    auth=(WORDPRESS_USER, WORDPRESS_APP_PASSWORD),
                    headers={'Content-Disposition': f'attachment; filename={image_path.name}'},
                    files={'file': (image_path.name, img, 'image/jpeg')}
                )
                res.raise_for_status()
                return res.json()
        except Exception as e:
            print(f"âš ï¸ ë¯¸ë””ì–´ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def is_already_posted_on_wp(self, original_url: str) -> bool:
        """WordPressì—ì„œ ì›ë¬¸ URL ê¸°ì¤€ ì¤‘ë³µ ê²Œì‹œ ì—¬ë¶€ í™•ì¸"""
        try:
            search_term = original_url.split('/')[-2] if original_url.endswith('/') else original_url.split('/')[-1]
            res = requests.get(
                f"{self.wordpress_api}/posts",
                auth=(WORDPRESS_USER, WORDPRESS_APP_PASSWORD),
                params={'search': search_term, 'per_page': 5, 'status': 'any'},
                timeout=10
            )
            if res.status_code == 200:
                for post in res.json():
                    if original_url in post.get('content', {}).get('rendered', ''):
                        print(f"âš ï¸ ì¤‘ë³µ ê°ì§€ â†’ ìŠ¤í‚µ: {post['link']}")
                        return True
            return False
        except Exception as e:
            print(f"âš ï¸ ì¤‘ë³µ ì²´í¬ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {e}")
            return False

    def commit_posted_articles(self):
        """posted_articles.json git ì»¤ë°‹ (ìºì‹œ ìœ ì‹¤ ë°©ì§€)"""
        try:
            import subprocess
            subprocess.run(['git', 'config', 'user.email', 'action@github.com'], check=True)
            subprocess.run(['git', 'config', 'user.name', 'GitHub Action'], check=True)
            subprocess.run(['git', 'add', POSTED_ARTICLES_FILE], check=True)
            result = subprocess.run(['git', 'diff', '--cached', '--quiet'], capture_output=True)
            if result.returncode != 0:
                subprocess.run(
                    ['git', 'commit', '-m', f'chore: update posted_articles [{datetime.now().strftime("%Y-%m-%d %H:%M")}]'],
                    check=True
                )
                subprocess.run(['git', 'push'], check=True)
                print("ğŸ“ posted_articles.json â†’ git ì»¤ë°‹ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ git ì»¤ë°‹ ì‹¤íŒ¨ (ìºì‹œë¡œ ëŒ€ì²´): {e}")

    def post_to_wordpress(self, title: str, content: str, slug: str,
                           featured_media_id: int, original_date: datetime,
                           excerpt: str = "", status: str = "publish") -> bool:
        post_data = {
            'title': title,
            'content': content,
            'slug': slug,
            'status': status,
            'featured_media': featured_media_id or 0,
            'date': original_date.strftime('%Y-%m-%dT%H:%M:%S')
        }
        if excerpt:
            post_data['excerpt'] = excerpt
        try:
            res = requests.post(
                f"{self.wordpress_api}/posts",
                auth=(WORDPRESS_USER, WORDPRESS_APP_PASSWORD),
                json=post_data
            )
            res.raise_for_status()
            post_info = res.json()
            label = "ğŸ“ ì„ì‹œì €ì¥" if status == "draft" else "âœ¨ ê²Œì‹œ ì„±ê³µ"
            print(f"{label}: {post_info['link']}")
            return True
        except Exception as e:
            print(f"âŒ ê²Œì‹œ ì‹¤íŒ¨: {e}")
            if hasattr(e, 'response') and e.response is not None:
                print(f"   {e.response.text[:300]}")
            return False

    def process_article(self, article: dict) -> bool:
        print(f"\n{'='*60}")
        print(f"ğŸ“° {article['title'][:70]}")
        print(f"ğŸ“… {article['date'].strftime('%Y-%m-%d %H:%M')}")
        print(f"{'='*60}")

        # 1. ì¤‘ë³µ ì²´í¬ (2ì¤‘ ì•ˆì „ë§)
        if not FORCE_UPDATE and self.is_already_posted_on_wp(article['link']):
            if article['link'] not in self.posted_articles:
                self.posted_articles.append(article['link'])
                self.save_posted_articles()
            return False

        # 2. ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘
        raw_html = self.fetch_full_content(article['link'])
        if not raw_html:
            print("âš ï¸ ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ â†’ ìŠ¤í‚µ")
            return False

        # 3. Gemini ì œëª© ë²ˆì—­ + SEO í¸ì§‘
        print("ğŸ”„ [1ë‹¨ê³„] Gemini ì œëª© ë²ˆì—­+í¸ì§‘ ì¤‘...")
        title_ko = self.gemini.translate_and_edit_title(article['title'])

        # 4. Gemini ë³¸ë¬¸ ë²ˆì—­ + SEO í¸ì§‘
        print("âœï¸  [2ë‹¨ê³„] Gemini ë³¸ë¬¸ ë²ˆì—­+í¸ì§‘ ì¤‘...")
        content_ko = self.gemini.translate_and_edit_content(raw_html)

        # 5. excerpt ìƒì„±
        excerpt = ""
        if GENERATE_EXCERPT:
            print("ğŸ“‹ [3ë‹¨ê³„] excerpt ìƒì„± ì¤‘...")
            excerpt = self.gemini.generate_excerpt(title_ko, content_ko)
            time.sleep(1)

        # 6. Slug ìƒì„±
        slug = self.generate_slug(article['title'], article['date'])
        print(f"ğŸ”— Slug: {slug}")

        # 7. ì´ë¯¸ì§€ ì²˜ë¦¬
        print("ğŸ” ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘...")
        featured_id = 0
        img_url = self.get_main_image_url(article['link'])
        if img_url:
            local_img = self.download_image(img_url)
            if local_img:
                media_info = self.upload_media(local_img)
                if media_info:
                    featured_id = media_info['id']
                try:
                    local_img.unlink()
                except:
                    pass

        # 8. ìµœì¢… ë³¸ë¬¸ êµ¬ì„± + ì›ë¬¸ ì¶œì²˜
        final_content = content_ko
        final_content += (
            "\n\n<hr style='margin:40px 0 20px 0;border:0;border-top:1px solid #e0e0e0;'>\n"
            f"<p style='font-size:13px;color:#777;'>"
            f"<strong>ì›ë¬¸:</strong> "
            f"<a href='{article['link']}' target='_blank' rel='noopener'>{article['title']}</a>"
            f"</p>"
        )

        # 9. WordPress ê²Œì‹œ
        label = "draft(ì„ì‹œì €ì¥)" if POST_STATUS == "draft" else "publish(ì¦‰ì‹œê³µê°œ)"
        print(f"ğŸ“¤ [4ë‹¨ê³„] WordPress {label} ì¤‘...")
        if self.post_to_wordpress(title_ko, final_content, slug, featured_id,
                                   article['date'], excerpt=excerpt, status=POST_STATUS):
            if not FORCE_UPDATE:
                self.posted_articles.append(article['link'])
                self.save_posted_articles()
            return True
        return False

    def run(self):
        print(f"\n{'='*60}")
        print(f"pronews.jp â†’ prodg.kr ìë™ ë²ˆì—­ v6")
        print(f"ì—”ì§„: Gemini ë‹¨ì¼ ({GEMINI_MODEL})")
        print(f"ê²Œì‹œ: {POST_STATUS.upper()} ({'ì¦‰ì‹œ ê³µê°œ' if POST_STATUS == 'publish' else 'ì„ì‹œì €ì¥ â†’ ìˆ˜ë™ ê²€ìˆ˜'})")
        print(f"ì¼ì¼ í•œë„: ìµœëŒ€ {DAILY_LIMIT}ê±´")
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*60}\n")

        if not WORDPRESS_USER or not WORDPRESS_APP_PASSWORD:
            print("âŒ WP_USER / WP_APP_PASSWORD í™˜ê²½ë³€ìˆ˜ í•„ìš”")
            sys.exit(1)

        articles = self.fetch_rss_feed()
        if not articles:
            print("âœ… ì²˜ë¦¬í•  ê¸°ì‚¬ ì—†ìŒ (ëª¨ë‘ ê²Œì‹œ ì™„ë£Œ)")
            return

        success = 0
        for i, article in enumerate(articles, 1):
            print(f"\n[{i}/{len(articles)}]")
            if self.process_article(article):
                success += 1
            time.sleep(3)

        print(f"\n{'='*60}")
        print(f"ğŸ ì™„ë£Œ: {success}/{len(articles)}ê±´ ê²Œì‹œ")
        print(f"{'='*60}\n")

        if success > 0:
            self.commit_posted_articles()


if __name__ == "__main__":
    bot = NewsTranslator()
    bot.run()
