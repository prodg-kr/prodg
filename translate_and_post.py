#!/usr/bin/env python3
"""
pronews.jp ìë™ ë²ˆì—­ ì‹œìŠ¤í…œ v4
íŒŒì´í”„ë¼ì¸: ì¼ë³¸ì–´ ì›ë¬¸ â†’ Groq 1ì°¨ ë²ˆì—­ â†’ Gemini Flash 2ì°¨ SEO í¸ì§‘ â†’ WordPress ê²Œì‹œ

v3 â†’ v4 ë³€ê²½ì‚¬í•­:
- googletrans ì œê±° â†’ Groq API (llama-3.3-70b, ë¬´ë£Œ, ì•ˆì •ì )
- 2ì°¨ SEO í¸ì§‘ ì¶”ê°€ â†’ Gemini 2.5 Flash (ë¬´ë£Œ í”Œëœ)
- í•˜ë£¨ ìµœëŒ€ 10ê±´ ì œí•œ (ìµœì‹  ê¸°ì‚¬ ìš°ì„ , ë¶€ì¡±í•˜ë©´ ê³¼ê±° ë¯¸ê²Œì‹œ ê¸°ì‚¬ë¡œ ì±„ì›€)
- ëª¨ë¸ëª… í™˜ê²½ë³€ìˆ˜ë¡œ êµì²´ ê°€ëŠ¥ (GROQ_MODEL, GEMINI_MODEL)
"""

import os
import sys
import requests
import feedparser
from datetime import datetime
from pathlib import Path
import json
import time
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
import hashlib
import re

# ==========================================
# ì„¤ì •
# ==========================================
WORDPRESS_URL          = "https://prodg.kr"
WORDPRESS_USER         = os.environ.get("WP_USER")
WORDPRESS_APP_PASSWORD = os.environ.get("WP_APP_PASSWORD")
GROQ_API_KEY           = os.environ.get("GROQ_API_KEY")
GEMINI_API_KEY         = os.environ.get("GEMINI_API_KEY")
PRONEWS_RSS            = "https://jp.pronews.com/feed"
POSTED_ARTICLES_FILE   = "posted_articles.json"
FORCE_UPDATE           = os.environ.get("FORCE_UPDATE", "false").lower() == "true"
DAILY_LIMIT            = 10  # í•˜ë£¨ ìµœëŒ€ ê²Œì‹œ ê±´ìˆ˜

# ëª¨ë¸ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ë¡œ ì–¸ì œë“  êµì²´ ê°€ëŠ¥)
GROQ_MODEL   = os.environ.get("GROQ_MODEL",   "llama-3.3-70b-versatile")
GEMINI_MODEL = os.environ.get("GEMINI_MODEL", "gemini-2.5-flash")

# API ì—”ë“œí¬ì¸íŠ¸
GROQ_API_URL   = "https://api.groq.com/openai/v1/chat/completions"
GEMINI_API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_MODEL}:generateContent"


# ==========================================
# Groq ë²ˆì—­ê¸° (1ì°¨: ì¼ë³¸ì–´ â†’ í•œêµ­ì–´ ì§ì—­)
# ==========================================
class GroqTranslator:
    """
    Groq APIë¡œ ì¼ë³¸ì–´ â†’ í•œêµ­ì–´ ë²ˆì—­
    - ëª¨ë¸: llama-3.3-70b-versatile (ë¬´ë£Œ, ë¶„ë‹¹ 30íšŒ, ì¼ 14,400íšŒ)
    - ì—­í• : ë¹ ë¥´ê³  ì •í™•í•œ ì§ì—­ (SEO í¸ì§‘ì€ Geminiê°€ ë‹´ë‹¹)
    - HTML ì²˜ë¦¬: íƒœê·¸ ì œê±° í›„ í…ìŠ¤íŠ¸ë§Œ ë²ˆì—­, ë‹¨ë½ êµ¬ì¡° ìœ ì§€
    """

    def __init__(self):
        self.api_key = GROQ_API_KEY
        if not self.api_key:
            print("âŒ GROQ_API_KEY ë¯¸ì„¤ì •")
            sys.exit(1)

    def _call_api(self, messages: list, max_tokens: int = 4096) -> str:
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        payload = {
            "model": GROQ_MODEL,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": 0.3  # ë²ˆì—­ì€ ë‚®ì€ temperature (ì¼ê´€ì„± ìš°ì„ )
        }
        try:
            res = requests.post(GROQ_API_URL, headers=headers, json=payload, timeout=60)
            res.raise_for_status()
            return res.json()["choices"][0]["message"]["content"].strip()
        except Exception as e:
            print(f"âš ï¸ Groq API ì˜¤ë¥˜: {e}")
            return ""

    def translate_title(self, title_ja: str) -> str:
        """ì œëª© ë²ˆì—­"""
        messages = [
            {
                "role": "system",
                "content": (
                    "You are a professional Japanese to Korean translator specializing in "
                    "video production and camera industry news. "
                    "Translate the given Japanese title to Korean accurately. "
                    "Output only the translated title, nothing else."
                )
            },
            {"role": "user", "content": f"ë‹¤ìŒ ì¼ë³¸ì–´ ì œëª©ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”:\n{title_ja}"}
        ]
        result = self._call_api(messages, max_tokens=200)
        return result if result else title_ja

    def translate_content(self, html_content: str) -> str:
        """
        ë³¸ë¬¸ ë²ˆì—­
        - HTMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ â†’ ì²­í¬ ë¶„í•  ë²ˆì—­ â†’ HTML ì¬ì¡°ë¦½
        - ì´ë¯¸ì§€/í—¤ë” íƒœê·¸ëŠ” í”Œë ˆì´ìŠ¤í™€ë”ë¡œ ë³´ì¡´
        """
        if not html_content:
            return ""

        soup = BeautifulSoup(html_content, 'lxml')

        # ì´ë¯¸ì§€ íƒœê·¸ ë³´ì¡´
        images = {}
        for i, img in enumerate(soup.find_all('img')):
            placeholder = f"___IMG_{i}___"
            images[placeholder] = str(img)
            img.replace_with(placeholder)

        # í—¤ë” íƒœê·¸ ë³´ì¡´
        headers_map = {}
        for i, tag in enumerate(soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])):
            placeholder = f"___H{i}_{tag.name}___"
            headers_map[placeholder] = {'tag': tag.name, 'text': tag.get_text(strip=True)}
            tag.replace_with(placeholder)

        # ë‹¨ë½ ë‹¨ìœ„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        paragraphs = []
        for elem in soup.find_all(['p', 'li', 'blockquote']):
            text = elem.get_text(separator=' ', strip=True)
            if text and len(text) > 5:
                paragraphs.append(text)

        if not paragraphs:
            full_text = soup.get_text(separator='\n', strip=True)
            paragraphs = [line for line in full_text.split('\n') if line.strip()]

        # ì²­í¬ ë‹¨ìœ„ ë²ˆì—­ (ì²­í¬ë‹¹ ìµœëŒ€ 2000ì)
        translated_paragraphs = []
        chunk, chunk_size = [], 0

        for para in paragraphs:
            if chunk_size + len(para) > 2000 and chunk:
                translated = self._translate_chunk('\n\n'.join(chunk))
                translated_paragraphs.extend(translated.split('\n\n'))
                chunk, chunk_size = [], 0
                time.sleep(0.5)
            chunk.append(para)
            chunk_size += len(para)

        if chunk:
            translated = self._translate_chunk('\n\n'.join(chunk))
            translated_paragraphs.extend(translated.split('\n\n'))

        # HTML ì¬ì¡°ë¦½
        translated_html = ""
        for para in translated_paragraphs:
            para = para.strip()
            if not para:
                continue
            if para.startswith('___'):
                translated_html += para + "\n"
            else:
                translated_html += f"<p>{para}</p>\n"

        # í—¤ë” íƒœê·¸ ë³µì›
        for placeholder, info in headers_map.items():
            if placeholder in translated_html:
                header_ko = self._translate_chunk(info['text']) if info['text'] else info['text']
                translated_html = translated_html.replace(
                    placeholder,
                    f"<{info['tag']}>{header_ko}</{info['tag']}>"
                )

        # ì´ë¯¸ì§€ íƒœê·¸ ë³µì›
        for placeholder, img_tag in images.items():
            translated_html = translated_html.replace(placeholder, img_tag)

        return translated_html

    def _translate_chunk(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì²­í¬ ë²ˆì—­"""
        if not text.strip():
            return text
        messages = [
            {
                "role": "system",
                "content": (
                    "You are a professional Japanese to Korean translator specializing in "
                    "video production, broadcasting, and camera industry content. "
                    "Translate accurately while preserving paragraph structure. "
                    "Keep technical terms, product names, model numbers, and brand names as-is. "
                    "Keep placeholders like ___IMG_0___ or ___H0_h2___ unchanged. "
                    "Output only the translated text, nothing else."
                )
            },
            {"role": "user", "content": f"ë‹¤ìŒ ì¼ë³¸ì–´ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”:\n\n{text}"}
        ]
        result = self._call_api(messages, max_tokens=4096)
        return result if result else text


# ==========================================
# Gemini SEO í¸ì§‘ê¸° (2ì°¨: ì§ì—­ â†’ SEO ìµœì í™”)
# ==========================================
class GeminiEditor:
    """
    Gemini 2.5 Flashë¡œ ë²ˆì—­ëœ í•œêµ­ì–´ë¥¼ SEO ìµœì í™” í¸ì§‘
    - ì—­í• : ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ìœ¤ë¬¸ + SEO ì œëª© ì¬ì‘ì„± + ì „ë¬¸ìš©ì–´ ë³´ì •
    - ë¹„ìš©: ë¬´ë£Œ í”Œëœ (3ê°œì›”), 10ê±´/ì¼ Ã— 2í˜¸ì¶œ = 20íšŒ/ì¼ (í•œë„ 500íšŒ ëŒ€ë¹„ ì—¬ìœ )
    - ëª¨ë¸ ë³€ê²½: GEMINI_MODEL í™˜ê²½ë³€ìˆ˜ë¡œ êµì²´ ê°€ëŠ¥
    """

    def __init__(self):
        self.api_key = GEMINI_API_KEY
        self.enabled = bool(self.api_key)
        if not self.enabled:
            print("âš ï¸ GEMINI_API_KEY ë¯¸ì„¤ì • â†’ SEO í¸ì§‘ ê±´ë„ˆëœ€ (Groq ë²ˆì—­ ê²°ê³¼ë§Œ ì‚¬ìš©)")

    def _call_api(self, prompt: str, max_tokens: int = 2048) -> str:
        if not self.enabled:
            return ""
        # GEMINI_MODELì´ í™˜ê²½ë³€ìˆ˜ë¡œ ë³€ê²½ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë§¤ í˜¸ì¶œì‹œ URL ì¬ìƒì„±
        url = (
            f"https://generativelanguage.googleapis.com/v1beta/models/"
            f"{GEMINI_MODEL}:generateContent?key={self.api_key}"
        )
        payload = {
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {
                "maxOutputTokens": max_tokens,
                "temperature": 0.7
            }
        }
        try:
            res = requests.post(url, json=payload, timeout=60)
            res.raise_for_status()
            candidates = res.json().get("candidates", [])
            if candidates:
                return candidates[0]["content"]["parts"][0]["text"].strip()
            return ""
        except Exception as e:
            print(f"âš ï¸ Gemini API ì˜¤ë¥˜: {e}")
            return ""

    def edit_title(self, title_ko: str, title_ja: str) -> str:
        """ì œëª© SEO í¸ì§‘ - í•µì‹¬ í‚¤ì›Œë“œ ì•ë°°ì¹˜, 30ì ë‚´ì™¸"""
        if not self.enabled:
            return title_ko

        prompt = f"""ë‹¹ì‹ ì€ ì˜ìƒ/ì¹´ë©”ë¼ ì „ë¬¸ ë¯¸ë””ì–´ì˜ SEO ì—ë””í„°ì…ë‹ˆë‹¤.

ì¼ë³¸ì–´ ì›ì œ: {title_ja}
ë²ˆì—­ëœ ì œëª©: {title_ko}

êµ¬ê¸€ ê²€ìƒ‰ ìµœì í™”ëœ í•œêµ­ì–´ ì œëª©ì„ ì‘ì„±í•˜ì„¸ìš”.

ê·œì¹™:
1. í•µì‹¬ ì œí’ˆëª…/ë¸Œëœë“œëª… ë°˜ë“œì‹œ í¬í•¨ (Sony, Canon, DJI, Blackmagic, DaVinci ë“± ì›ë¬¸ í‘œê¸° ìœ ì§€)
2. ê²€ìƒ‰ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì•ìª½ì— ë°°ì¹˜
3. ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ (ì§ì—­ì²´, ì–´ìƒ‰í•œ ì¡°ì‚¬ ê¸ˆì§€)
4. 30ì ë‚´ì™¸ (ìµœëŒ€ 40ì)
5. ì œëª©ë§Œ ì¶œë ¥ (ì„¤ëª…, ë”°ì˜´í‘œ, ë²ˆí˜¸ ì—†ìŒ)"""

        result = self._call_api(prompt, max_tokens=100)
        if result:
            result = re.sub(r'^[\d\.\)\-\s"\'ã€Œã€]+', '', result).strip().strip('"\'ã€Œã€')
            print(f"   âœï¸ SEO ì œëª©: {result}")
            return result
        return title_ko

    def edit_content(self, content_ko: str) -> str:
        """ë³¸ë¬¸ SEO í¸ì§‘ - ì§ì—­ì²´ ìœ¤ë¬¸, ì „ë¬¸ìš©ì–´ ë³´ì •, HTML íƒœê·¸ ìœ ì§€"""
        if not self.enabled or not content_ko:
            return content_ko

        if len(content_ko) <= 3000:
            return self._edit_chunk(content_ko)

        # ì¥ë¬¸ì€ <p> íƒœê·¸ ê¸°ì¤€ ì²­í¬ ë¶„í• 
        chunks = self._split_html_chunks(content_ko, max_chars=3000)
        edited_chunks = []
        for i, chunk in enumerate(chunks):
            print(f"   ğŸ“ Gemini í¸ì§‘ ì¤‘... ({i+1}/{len(chunks)})")
            edited = self._edit_chunk(chunk)
            edited_chunks.append(edited if edited else chunk)
            time.sleep(1)
        return "\n".join(edited_chunks)

    def _edit_chunk(self, html_chunk: str) -> str:
        prompt = f"""ë‹¹ì‹ ì€ ì˜ìƒ/ì¹´ë©”ë¼ ì „ë¬¸ ë¯¸ë””ì–´ì˜ í•œêµ­ì–´ ì—ë””í„°ì…ë‹ˆë‹¤.

ì•„ë˜ëŠ” ì¼ë³¸ì–´ ê¸°ì‚¬ë¥¼ AIê°€ ë²ˆì—­í•œ í•œêµ­ì–´ HTML ë³¸ë¬¸ì…ë‹ˆë‹¤.
ì§ì—­ì²´ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ìœ¤ë¬¸í•˜ê³  SEOë¥¼ ìµœì í™”í•˜ì„¸ìš”.

í¸ì§‘ ê·œì¹™:
1. HTML íƒœê·¸(<p>, <h2>, <h3>, <img> ë“±)ëŠ” ë°˜ë“œì‹œ ê·¸ëŒ€ë¡œ ìœ ì§€
2. ì§ì—­ì²´, ì–´ìƒ‰í•œ ì¡°ì‚¬, ì¼ë³¸ì‹ í‘œí˜„ì„ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ìˆ˜ì •
3. ë¬¸ì²´ëŠ” ë°˜ë“œì‹œ '~í•©ë‹ˆë‹¤', '~í–ˆìŠµë‹ˆë‹¤', '~ì…ë‹ˆë‹¤' ë“± í•©ì‡¼ì²´(ê²©ì‹ì²´)ë¡œ í†µì¼
   - '~í•œë‹¤', '~í–ˆë‹¤', '~ì´ë‹¤' ë“± í‰ì„œì²´ ì‚¬ìš© ê¸ˆì§€
   - '~í•´ìš”', '~ì˜ˆìš”' ë“± í•´ìš”ì²´ ì‚¬ìš© ê¸ˆì§€
4. ì˜ìƒ/ì¹´ë©”ë¼ ì „ë¬¸ìš©ì–´ ì •í™•íˆ í‘œê¸°:
   - ë¸Œëœë“œëª…: Sony, Canon, Nikon, DJI, Blackmagic, DaVinci Resolve ë“± ì›ë¬¸ ìœ ì§€
   - í•´ìƒë„: 4K, 8K, Full HD
   - í”„ë ˆì„ë ˆì´íŠ¸: fps, 24p, 60p
   - ê¸°íƒ€: ì½”ë±, ë¹„íŠ¸ë ˆì´íŠ¸, ì¡°ë¦¬ê°œ, ì…”í„°ìŠ¤í”¼ë“œ ë“± ì •í™•í•œ í•œêµ­ì–´ ì‚¬ìš©
5. ë‹¨ë½ êµ¬ì¡°ì™€ ë¬¸ì¥ ìˆ˜ ìœ ì§€ (ë‚´ìš© ì¶”ê°€/ì‚­ì œ ê¸ˆì§€)
6. HTMLë§Œ ì¶œë ¥ (ì„¤ëª… í…ìŠ¤íŠ¸ ì—†ìŒ)

ë²ˆì—­ëœ HTML:
{html_chunk}"""

        result = self._call_api(prompt, max_tokens=4096)
        return result if result else html_chunk

    def _split_html_chunks(self, html: str, max_chars: int = 3000) -> list:
        """<p> íƒœê·¸ ê²½ê³„ ê¸°ì¤€ìœ¼ë¡œ HTML ì²­í¬ ë¶„í• """
        chunks = []
        current_chunk = ""
        parts = re.split(r'(?=<p>)', html)
        for part in parts:
            if len(current_chunk) + len(part) > max_chars and current_chunk:
                chunks.append(current_chunk)
                current_chunk = part
            else:
                current_chunk += part
        if current_chunk:
            chunks.append(current_chunk)
        return chunks if chunks else [html]


# ==========================================
# ë©”ì¸ ë²ˆì—­ ì‹œìŠ¤í…œ
# ==========================================
class NewsTranslator:
    def __init__(self):
        self.groq = GroqTranslator()
        self.gemini = GeminiEditor()
        self.wordpress_api = f"{WORDPRESS_URL}/wp-json/wp/v2"
        self.posted_articles = self.load_posted_articles()

    def load_posted_articles(self) -> list:
        if Path(POSTED_ARTICLES_FILE).exists():
            with open(POSTED_ARTICLES_FILE, 'r') as f:
                try:
                    return json.load(f)
                except:
                    return []
        return []

    def save_posted_articles(self):
        with open(POSTED_ARTICLES_FILE, 'w') as f:
            json.dump(self.posted_articles, f, indent=2)

    def fetch_rss_feed(self) -> list:
        """
        RSS í”¼ë“œì—ì„œ ë¯¸ê²Œì‹œ ê¸°ì‚¬ ì¡°íšŒ
        - ìµœì‹ ìˆœ ì •ë ¬
        - ìµœì‹  ê¸°ì‚¬ê°€ 10ê±´ ë¯¸ë§Œì´ë©´ ê³¼ê±° ë¯¸ê²Œì‹œ ê¸°ì‚¬ë¡œ ì±„ì›Œ í•­ìƒ ìµœëŒ€ 10ê±´ ë°˜í™˜
        """
        print(f"ğŸ“¡ RSS í”¼ë“œ í™•ì¸ ì¤‘: {PRONEWS_RSS}")
        feed = feedparser.parse(PRONEWS_RSS)
        print(f"ğŸ” ì´ {len(feed.entries)}ê°œ í”¼ë“œ í•­ëª© ê²€ìƒ‰...")

        unposted = []
        for entry in feed.entries:
            if not FORCE_UPDATE and entry.link in self.posted_articles:
                continue
            try:
                article_date = datetime(*entry.published_parsed[:6])
            except:
                article_date = datetime.now()

            unposted.append({
                'title': entry.title,
                'link': entry.link,
                'date': article_date
            })

        # ìµœì‹ ìˆœ ì •ë ¬ í›„ ìµœëŒ€ 10ê±´ (ìµœì‹  + ê³¼ê±° ë¯¸ê²Œì‹œ ìˆœì„œë¡œ ìë™ ì±„ì›Œì§)
        unposted.sort(key=lambda x: x['date'], reverse=True)
        target = unposted[:DAILY_LIMIT]

        print(f"âœ… ë¯¸ê²Œì‹œ ê¸°ì‚¬: {len(unposted)}ê±´ â†’ ì˜¤ëŠ˜ ì²˜ë¦¬: {len(target)}ê±´ (ìµœëŒ€ {DAILY_LIMIT}ê±´)")
        return target

    def fetch_full_content(self, url: str):
        """ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘ + ë¶ˆí•„ìš” ìš”ì†Œ ì œê±°"""
        try:
            print(f"ğŸ“„ ìŠ¤í¬ë˜í•‘: {url}")
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'lxml')
            content_div = (
                soup.find('div', class_='entry-content') or
                soup.find('div', class_='post-content') or
                soup.find('div', class_='article-content') or
                soup.find('article')
            )
            if not content_div:
                return None

            # ë¶ˆí•„ìš” í…ìŠ¤íŠ¸/ì„¹ì…˜ ì œê±°
            for elem in content_div.find_all(string=re.compile(
                r'ì›ë¬¸ ê²Œì‹œì‹œê°:|ì¶œì²˜:|åŸæ–‡æ²è¼‰æ™‚åˆ»:|ã‚½ãƒ¼ã‚¹:|ãƒãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼|é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰|ã“ã®è¨˜äº‹ã‚’ã‚·ã‚§ã‚¢|FOLLOW US'
            )):
                parent = elem.find_parent()
                if parent:
                    parent.decompose()

            remove_headings = ['ë°± ë„˜ë²„', 'é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'ãƒãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼',
                               'ã“ã®è¨˜äº‹ã‚’ã‚·ã‚§ã‚¢', 'ì´ ê¸°ì‚¬ ê³µìœ ', 'FOLLOW US', 'é–¢é€£è¨˜äº‹', 'ê´€ë ¨ ê¸°ì‚¬']
            for h_tag in content_div.find_all(['h2', 'h3', 'h4']):
                if any(kw in h_tag.get_text(strip=True) for kw in remove_headings):
                    next_elem = h_tag.find_next_sibling()
                    h_tag.decompose()
                    while next_elem and next_elem.name not in ['h1', 'h2', 'h3', 'h4']:
                        temp = next_elem.find_next_sibling()
                        next_elem.decompose()
                        next_elem = temp

            for tag in content_div(['script', 'style', 'iframe', 'noscript', 'form',
                                    'nav', 'aside', 'footer', 'header']):
                tag.decompose()

            social_classes = ['social-share', 'share-buttons', 'sns-share', 'social-links',
                               'share-links', 'addtoany', 'sharedaddy', 'jp-relatedposts',
                               'entry-footer', 'post-tags', 'post-categories', 'post-meta']
            for elem in content_div.find_all(class_=lambda x: x and any(
                sc in ' '.join(x).lower() for sc in social_classes
            )):
                elem.decompose()

            remove_keywords = [
                'facebook.com', 'twitter.com', 'line.me', 'instagram.com',
                'youtube.com', 'pronews.jp', 'kr.pronews.com', '/fellowship/',
                'getpocket.com', 'hatena.ne.jp', '/feed', '/columntitle/',
                '/specialtitle/', '/writer/', 'jp.pronews.com'
            ]
            for a in list(content_div.find_all('a')):
                href = a.get('href', '')
                text = a.get_text(strip=True)
                if any(kw in href.lower() for kw in remove_keywords) or href.startswith('//') or not text:
                    a.decompose()

            for tag_name in ['p', 'div', 'span', 'li']:
                for tag in content_div.find_all(tag_name):
                    if not tag.get_text(strip=True) and not tag.find('img'):
                        tag.decompose()

            return str(content_div)

        except Exception as e:
            print(f"âš ï¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return None

    def generate_slug(self, title_ja: str, article_date: datetime) -> str:
        """ì˜ë¬¸ slug ìƒì„± (ì˜ë¬¸ í‚¤ì›Œë“œ + ë‚ ì§œ)"""
        words = title_ja.split()
        slug_words = []
        for word in words[:6]:
            cleaned = re.sub(r'[^a-zA-Z0-9\-]', '', word.lower())
            if cleaned and len(cleaned) > 2:
                slug_words.append(cleaned)

        date_str = article_date.strftime('%Y%m%d')
        slug = ('-'.join(slug_words[:4]) + f"-{date_str}") if slug_words else f"article-{date_str}-{int(time.time())}"
        return slug[:60]

    def get_main_image_url(self, link: str):
        try:
            res = requests.get(link, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
            soup = BeautifulSoup(res.text, 'lxml')
            og_img = soup.find('meta', property='og:image')
            if og_img and og_img.get('content'):
                return og_img['content']
            content = soup.find('div', class_='entry-content')
            if content:
                img = content.find('img')
                if img and img.get('src'):
                    img_url = img['src']
                    return img_url if img_url.startswith('http') else urljoin(link, img_url)
        except:
            pass
        return None

    def download_image(self, url: str):
        if not url:
            return None
        try:
            print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ: {url}")
            res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=15)
            res.raise_for_status()
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
            original_filename = os.path.basename(urlparse(url).path).split('?')[0]
            ext = os.path.splitext(original_filename)[1]
            if ext not in ['.jpg', '.jpeg', '.png', '.gif', '.webp']:
                ext = '.jpg'
            path = Path(f"/tmp/pronews_{int(time.time())}_{url_hash}{ext}")
            with open(path, 'wb') as f:
                f.write(res.content)
            print(f"   âœ… {path.name}")
            return path
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def upload_media(self, image_path: Path):
        if not image_path or not image_path.exists():
            return None
        try:
            with open(image_path, 'rb') as img:
                res = requests.post(
                    f"{self.wordpress_api}/media",
                    auth=(WORDPRESS_USER, WORDPRESS_APP_PASSWORD),
                    headers={'Content-Disposition': f'attachment; filename={image_path.name}'},
                    files={'file': (image_path.name, img, 'image/jpeg')}
                )
                res.raise_for_status()
                return res.json()
        except Exception as e:
            print(f"âš ï¸ ë¯¸ë””ì–´ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def is_already_posted_on_wp(self, original_url: str) -> bool:
        """
        WordPressì—ì„œ ì›ë¬¸ URL ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ê²Œì‹œ ì—¬ë¶€ í™•ì¸
        - posted_articles.json ìºì‹œ ì‹¤íŒ¨ ì‹œ 2ì°¨ ì•ˆì „ë§ ì—­í• 
        - ì›ë¬¸ ë§í¬ë¥¼ ë³¸ë¬¸ì— í¬í•¨í•˜ë¯€ë¡œ ê²€ìƒ‰ìœ¼ë¡œ ì°¾ì„ ìˆ˜ ìˆìŒ
        """
        try:
            # ì›ë¬¸ URLì˜ ì¼ë¶€ë¡œ WordPress ê²€ìƒ‰
            search_term = original_url.split('/')[-2] if original_url.endswith('/') else original_url.split('/')[-1]
            res = requests.get(
                f"{self.wordpress_api}/posts",
                auth=(WORDPRESS_USER, WORDPRESS_APP_PASSWORD),
                params={'search': search_term, 'per_page': 5, 'status': 'publish'},
                timeout=10
            )
            if res.status_code == 200:
                posts = res.json()
                for post in posts:
                    if original_url in post.get('content', {}).get('rendered', ''):
                        print(f"âš ï¸ ì¤‘ë³µ ê°ì§€ â†’ ìŠ¤í‚µ: {post['link']}")
                        return True
            return False
        except Exception as e:
            print(f"âš ï¸ ì¤‘ë³µ ì²´í¬ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {e}")
            return False  # ì˜¤ë¥˜ ì‹œ ê²Œì‹œ ì§„í–‰ (ë³´ìˆ˜ì  ì²˜ë¦¬)

    def commit_posted_articles(self):
        """
        posted_articles.jsonì„ git ì €ì¥ì†Œì— ì»¤ë°‹
        - GitHub Actions ìºì‹œ ëŒ€ì‹  gitìœ¼ë¡œ ì˜êµ¬ ë³´ì¡´
        - ìºì‹œê°€ ë‚ ì•„ê°€ë„ ì¤‘ë³µ ê²Œì‹œ ë°©ì§€
        """
        try:
            import subprocess
            subprocess.run(['git', 'config', 'user.email', 'action@github.com'], check=True)
            subprocess.run(['git', 'config', 'user.name', 'GitHub Action'], check=True)
            subprocess.run(['git', 'add', POSTED_ARTICLES_FILE], check=True)
            result = subprocess.run(
                ['git', 'diff', '--cached', '--quiet'],
                capture_output=True
            )
            if result.returncode != 0:  # ë³€ê²½ì‚¬í•­ ìˆì„ ë•Œë§Œ ì»¤ë°‹
                subprocess.run(
                    ['git', 'commit', '-m', f'chore: update posted_articles [{datetime.now().strftime("%Y-%m-%d %H:%M")}]'],
                    check=True
                )
                subprocess.run(['git', 'push'], check=True)
                print("ğŸ“ posted_articles.json â†’ git ì»¤ë°‹ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ git ì»¤ë°‹ ì‹¤íŒ¨ (ìºì‹œë¡œ ëŒ€ì²´): {e}")

    def post_to_wordpress(self, title: str, content: str, slug: str,
                           featured_media_id: int, original_date: datetime) -> bool:
        post_data = {
            'title': title,
            'content': content,
            'slug': slug,
            'status': 'publish',
            'featured_media': featured_media_id or 0,
            'date': original_date.strftime('%Y-%m-%dT%H:%M:%S')
        }
        try:
            res = requests.post(
                f"{self.wordpress_api}/posts",
                auth=(WORDPRESS_USER, WORDPRESS_APP_PASSWORD),
                json=post_data
            )
            res.raise_for_status()
            print(f"âœ¨ ê²Œì‹œ ì„±ê³µ: {res.json()['link']}")
            return True
        except Exception as e:
            print(f"âŒ ê²Œì‹œ ì‹¤íŒ¨: {e}")
            if hasattr(e, 'response') and e.response is not None:
                print(f"   {e.response.text[:300]}")
            return False

    def process_article(self, article: dict) -> bool:
        print(f"\n{'='*60}")
        print(f"ğŸ“° {article['title'][:60]}")
        print(f"ğŸ“… {article['date'].strftime('%Y-%m-%d %H:%M')}")
        print(f"{'='*60}")

        # 1. ì¤‘ë³µ ì²´í¬ (posted_articles.json + WordPress 2ì¤‘ í™•ì¸)
        if not FORCE_UPDATE and self.is_already_posted_on_wp(article['link']):
            if article['link'] not in self.posted_articles:
                self.posted_articles.append(article['link'])
                self.save_posted_articles()
            return False

        # 2. ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘
        raw_html = self.fetch_full_content(article['link'])
        if not raw_html:
            print("âš ï¸ ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ â†’ ìŠ¤í‚µ")
            return False

        # 3. Groq 1ì°¨ ë²ˆì—­
        print("ğŸ”„ [1ë‹¨ê³„] Groq ë²ˆì—­ ì¤‘...")
        title_ko_raw = self.groq.translate_title(article['title'])
        content_ko_raw = self.groq.translate_content(raw_html)
        print(f"   ë²ˆì—­ ì œëª©: {title_ko_raw}")

        # 4. Gemini 2ì°¨ SEO í¸ì§‘
        print("âœï¸  [2ë‹¨ê³„] Gemini SEO í¸ì§‘ ì¤‘...")
        title_ko = self.gemini.edit_title(title_ko_raw, article['title'])
        content_ko = self.gemini.edit_content(content_ko_raw)

        # 5. Slug ìƒì„±
        slug = self.generate_slug(article['title'], article['date'])
        print(f"ğŸ”— Slug: {slug}")

        # 6. ì´ë¯¸ì§€ ì²˜ë¦¬
        print("ğŸ” ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘...")
        featured_id = 0
        img_url = self.get_main_image_url(article['link'])
        if img_url:
            local_img = self.download_image(img_url)
            if local_img:
                media_info = self.upload_media(local_img)
                if media_info:
                    featured_id = media_info['id']
                try:
                    local_img.unlink()
                except:
                    pass

        # 7. ìµœì¢… ë³¸ë¬¸ êµ¬ì„± + ì›ë¬¸ ì¶œì²˜
        final_content = content_ko
        final_content += (
            "\n\n<hr style='margin:40px 0 20px 0;border:0;border-top:1px solid #e0e0e0;'>\n"
            f"<p style='font-size:13px;color:#777;'>"
            f"<strong>ì›ë¬¸:</strong> "
            f"<a href='{article['link']}' target='_blank' rel='noopener'>{article['title']}</a>"
            f"</p>"
        )

        # 8. WordPress ê²Œì‹œ
        print("ğŸ“¤ WordPress ê²Œì‹œ ì¤‘...")
        if self.post_to_wordpress(title_ko, final_content, slug, featured_id, article['date']):
            if not FORCE_UPDATE:
                self.posted_articles.append(article['link'])
                self.save_posted_articles()
            return True
        return False

    def run(self):
        print(f"\n{'='*60}")
        print(f"pronews.jp â†’ prodg.kr ìë™ ë²ˆì—­ v4")
        print(f"ë²ˆì—­: Groq ({GROQ_MODEL})")
        print(f"í¸ì§‘: Gemini ({GEMINI_MODEL})")
        print(f"ì¼ì¼ í•œë„: ìµœëŒ€ {DAILY_LIMIT}ê±´")
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*60}\n")

        if not WORDPRESS_USER or not WORDPRESS_APP_PASSWORD:
            print("âŒ WP_USER / WP_APP_PASSWORD í™˜ê²½ë³€ìˆ˜ í•„ìš”")
            sys.exit(1)

        articles = self.fetch_rss_feed()
        if not articles:
            print("âœ… ì²˜ë¦¬í•  ê¸°ì‚¬ ì—†ìŒ (ëª¨ë‘ ê²Œì‹œ ì™„ë£Œ)")
            return

        success = 0
        for i, article in enumerate(articles, 1):
            print(f"\n[{i}/{len(articles)}]")
            if self.process_article(article):
                success += 1
            time.sleep(3)

        print(f"\n{'='*60}")
        print(f"ğŸ ì™„ë£Œ: {success}/{len(articles)}ê±´ ê²Œì‹œ")
        print(f"{'='*60}\n")

        # ê²Œì‹œ ê¸°ë¡ git ì»¤ë°‹ (ìºì‹œ ìœ ì‹¤ ë°©ì§€)
        if success > 0:
            self.commit_posted_articles()


if __name__ == "__main__":
    bot = NewsTranslator()
    bot.run()
